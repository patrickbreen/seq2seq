{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take some RNN models from tensorflow seq2seq:\n",
    "\n",
    "TODO:\n",
    "\n",
    "- [x] For test data, show input batch words, output words, and predicted words (and print this to a file every so often)\n",
    "\n",
    "- [x] Switch to `embedding_rnn_seq2seq` model\n",
    "\n",
    "- [x] Capture word embeddings\n",
    "\n",
    "- Normalize word embeddings\n",
    "\n",
    "- [x] Implement VAE\n",
    "\n",
    "- Capture z's and latent loss, and minimize latent loss\n",
    "\n",
    "- [x] Get good summaries of Error, etc written to files during training\n",
    "\n",
    "- Any more finishing of model using original en -> fr data. Such as saving the tensorboard graph model.\n",
    "\n",
    "Figures:\n",
    "\n",
    "1. model diagram\n",
    "2. Training curves\n",
    "3. token embeddings\n",
    "4. visualize latent (color by sequence families or GO category)\n",
    "5. reconstruction (multiple seq alignment)\n",
    "\n",
    "## Now simply apply to protein data\n",
    "\n",
    "- Run protein data through the system using same tokenization machinery as for en -> fr translation\n",
    "\n",
    "- Have to change bucket sizes appropriatly\n",
    "\n",
    "- Tokenize 2 amino acids to get 400 total combos because easier to analyze embeddings. See if order matters ie does XY = YX ?\n",
    "\n",
    "- Color latent space by sequence attributes\n",
    "\n",
    "## (Maybe) do the same thing, but for protein structure \n",
    "\n",
    "- (going to require more work since no longer dealing with discrete tokens)\n",
    "\n",
    "- Have to encode real valued positions, orientations etc ......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def make_vae(x):\n",
    "  \"\"\" Takes a numeric tensor and returns the reconstructed mean tensor of same dims\n",
    "  (batch_size, input_dim)\"\"\"\n",
    "\n",
    "  # A lot of this code section is taken and modified from https://jmetzen.github.io/2015-11-27/vae.html\n",
    "\n",
    "  # hyper params:\n",
    "  n_hidden_recog_1=500 # 1st layer encoder neurons\n",
    "  n_hidden_recog_2=500 # 2nd layer encoder neurons\n",
    "\n",
    "  n_hidden_gener_1=500 # 1st layer decoder neurons\n",
    "  n_hidden_gener_2=500 # 2nd layer decoder neurons\n",
    "  n_input=size*2\n",
    "  n_z=20\n",
    "\n",
    "  # CREATE NETWORK\n",
    "\n",
    "  # 1) input placeholder\n",
    "\n",
    "  # 2) weights and biases variables\n",
    "  def xavier_init(fan_in, fan_out, constant=1): \n",
    "      \"\"\" Xavier initialization of network weights\"\"\"\n",
    "      # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "      low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "      high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "      return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "\n",
    "  wr_h1 = tf.Variable(xavier_init(n_input, n_hidden_recog_1))\n",
    "  wr_h2 = tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2))\n",
    "  wr_out_mean = tf.Variable(xavier_init(n_hidden_recog_2, n_z))\n",
    "  wr_out_log_sigma = tf.Variable(xavier_init(n_hidden_recog_2, n_z))\n",
    "\n",
    "  br_b1 = tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32))\n",
    "  br_b2 = tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32))\n",
    "  br_out_mean = tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "  br_out_log_sigma = tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "\n",
    "  wg_h1 = tf.Variable(xavier_init(n_z, n_hidden_gener_1))\n",
    "  wg_h2 = tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2))\n",
    "  wg_out_mean = tf.Variable(xavier_init(n_hidden_gener_2, n_input))\n",
    "  # wg_out_log_sigma = tf.Variable(xavier_init(n_hidden_gener_2, n_input))\n",
    "\n",
    "  bg_b1 = tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32))\n",
    "  bg_b2 = tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32))\n",
    "  bg_out_mean = tf.Variable(tf.zeros([n_input], dtype=tf.float32))\n",
    "\n",
    "  # 3) recognition network\n",
    "\n",
    "  # use recognition network to predict mean and (log) variance of (latent) Gaussian distribution z (n_z dimensional)\n",
    "  r_layer_1 = tf.nn.softplus(tf.add(tf.matmul(x, wr_h1), br_b1))\n",
    "  r_layer_2 = tf.nn.softplus(tf.add(tf.matmul(r_layer_1, wr_h2), br_b2))\n",
    "  z_mean = tf.add(tf.matmul(r_layer_2, wr_out_mean), br_out_mean)\n",
    "  z_sigma = tf.add(tf.matmul(r_layer_2, wr_out_log_sigma), br_out_log_sigma)\n",
    "\n",
    "  # 4) do sampling on recognition network to get latent variables\n",
    "  # draw one n_z dimensional sample (for each input in batch), from normal distribution\n",
    "  eps = tf.random_normal((batch_size, n_z), 0, 1, dtype=tf.float32)\n",
    "\n",
    "  # scale that set of samples by predicted mu and epsilon to get samples of z, the latent distribution\n",
    "  # z = mu + sigma*epsilon\n",
    "  latent_z = tf.add(z_mean, tf.mul(tf.sqrt(tf.exp(z_sigma)), eps), name=\"latent_z\")\n",
    "\n",
    "  # 5) use generator network to predict mean of Bernoulli distribution of reconstructed input\n",
    "  g_layer_1 = tf.nn.softplus(tf.add(tf.matmul(latent_z, wg_h1), bg_b1))\n",
    "  g_layer_2 = tf.nn.softplus(tf.add(tf.matmul(g_layer_1, wg_h2), bg_b2))\n",
    "  x_reconstr_mean = tf.nn.sigmoid(tf.add(tf.matmul(g_layer_2, wg_out_mean), bg_out_mean))\n",
    "\n",
    "  # 6) loss\n",
    "  latent_loss = tf.mul(-0.5, tf.reduce_sum(1 + z_sigma - tf.square(z_mean) - tf.exp(z_sigma), 1), name=\"latent_loss\")\n",
    "\n",
    "  return x_reconstr_mean, latent_z, latent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model components from tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "from seq2seq import sequence_loss, embedding_rnn_decoder\n",
    "\n",
    "def model_with_buckets(encoder_inputs, decoder_inputs, targets, weights,\n",
    "                       buckets, seq2seq, softmax_loss_function=None, name=None):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "  if len(encoder_inputs) < buckets[-1][0]:\n",
    "    raise ValueError(\"Length of encoder_inputs (%d) must be at least that of la\"\n",
    "                     \"st bucket (%d).\" % (len(encoder_inputs), buckets[-1][0]))\n",
    "  if len(targets) < buckets[-1][1]:\n",
    "    raise ValueError(\"Length of targets (%d) must be at least that of last\"\n",
    "                     \"bucket (%d).\" % (len(targets), buckets[-1][1]))\n",
    "  if len(weights) < buckets[-1][1]:\n",
    "    raise ValueError(\"Length of weights (%d) must be at least that of last\"\n",
    "                     \"bucket (%d).\" % (len(weights), buckets[-1][1]))\n",
    "\n",
    "  all_inputs = encoder_inputs + decoder_inputs + targets + weights\n",
    "  losses = []\n",
    "  outputs = []\n",
    "  latent_losses = []\n",
    "  latent_zs = []\n",
    "  with ops.op_scope(all_inputs, name, \"model_with_buckets\"):\n",
    "    for j, bucket in enumerate(buckets):\n",
    "      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                         reuse=True if j > 0 else None):\n",
    "        bucket_outputs, _ = seq2seq(encoder_inputs[:bucket[0]],\n",
    "                                    decoder_inputs[:bucket[1]])\n",
    "        outputs.append(bucket_outputs)\n",
    "#         latent_losses.append(latent_loss)\n",
    "#         latent_zs.append(latent_z)\n",
    "        \n",
    "        losses.append(sequence_loss(\n",
    "            outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n",
    "            softmax_loss_function=softmax_loss_function))\n",
    "\n",
    "  return outputs, losses\n",
    "\n",
    "\n",
    "def embedding_rnn_seq2seq(encoder_inputs, decoder_inputs, cell,\n",
    "                          num_encoder_symbols, num_decoder_symbols,\n",
    "                          embedding_size, output_projection=None,\n",
    "                          feed_previous=False, dtype=dtypes.float32,\n",
    "                          scope=None,\n",
    "                         ):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "  with variable_scope.variable_scope(scope or \"embedding_rnn_seq2seq\"):\n",
    "    # Encoder.\n",
    "    encoder_cell = rnn_cell.EmbeddingWrapper(\n",
    "        cell, embedding_classes=num_encoder_symbols,\n",
    "        embedding_size=embedding_size)\n",
    "    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n",
    "    \n",
    "    # Patrick: keep your eyes on that encoder state\n",
    "    encoder_state, latent_z, latent_loss = make_vae(encoder_state)\n",
    "\n",
    "    # Decoder.\n",
    "    if output_projection is None:\n",
    "      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n",
    "\n",
    "    if isinstance(feed_previous, bool):\n",
    "      return embedding_rnn_decoder(\n",
    "          decoder_inputs, encoder_state, cell, num_decoder_symbols,\n",
    "          embedding_size, output_projection=output_projection,\n",
    "          feed_previous=feed_previous)\n",
    "\n",
    "    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n",
    "    def decoder(feed_previous_bool):\n",
    "      reuse = None if feed_previous_bool else True\n",
    "      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                         reuse=reuse):\n",
    "        outputs, state = embedding_rnn_decoder(\n",
    "            decoder_inputs, encoder_state, cell, num_decoder_symbols,\n",
    "            embedding_size, output_projection=output_projection,\n",
    "            feed_previous=feed_previous_bool,\n",
    "            update_embedding_for_previous=False)\n",
    "        state_list = [state]\n",
    "        if nest.is_sequence(state):\n",
    "          state_list = nest.flatten(state)\n",
    "        return outputs + state_list\n",
    "\n",
    "    outputs_and_state = control_flow_ops.cond(feed_previous,\n",
    "                                              lambda: decoder(True),\n",
    "                                              lambda: decoder(False))\n",
    "    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n",
    "    state_list = outputs_and_state[outputs_len:]\n",
    "    state = state_list[0]\n",
    "    if nest.is_sequence(encoder_state):\n",
    "      state = nest.pack_sequence_as(structure=encoder_state, flat_sequence=state_list)\n",
    "    \n",
    "    return outputs_and_state[:outputs_len], state, latent_z, latent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The translation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import data_utils\n",
    "\n",
    "\n",
    "class Seq2SeqModel(object):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "  def __init__(self,\n",
    "               source_vocab_size,\n",
    "               target_vocab_size,\n",
    "               buckets,\n",
    "               size,\n",
    "               num_layers,\n",
    "               max_gradient_norm,\n",
    "               batch_size,\n",
    "               learning_rate,\n",
    "               learning_rate_decay_factor,\n",
    "               use_lstm=False,\n",
    "               num_samples=512,\n",
    "               forward_only=False,\n",
    "               dtype=tf.float32):\n",
    "    self.source_vocab_size = source_vocab_size\n",
    "    self.target_vocab_size = target_vocab_size\n",
    "    self.buckets = buckets\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = tf.Variable(\n",
    "        float(learning_rate), trainable=False, dtype=dtype)\n",
    "    self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "        self.learning_rate * learning_rate_decay_factor)\n",
    "    self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # If we use sampled softmax, we need an output projection.\n",
    "    output_projection = None\n",
    "    softmax_loss_function = None\n",
    "    # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "    if num_samples > 0 and num_samples < self.target_vocab_size:\n",
    "      w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\n",
    "      w = tf.transpose(w_t)\n",
    "      b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\n",
    "      output_projection = (w, b)\n",
    "\n",
    "      def sampled_loss(inputs, labels):\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "        # avoid numerical instabilities.\n",
    "        local_w_t = tf.cast(w_t, tf.float32)\n",
    "        local_b = tf.cast(b, tf.float32)\n",
    "        local_inputs = tf.cast(inputs, tf.float32)\n",
    "        return tf.cast(\n",
    "            tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n",
    "                                       num_samples, self.target_vocab_size),dtype)\n",
    "      \n",
    "      softmax_loss_function = sampled_loss\n",
    "\n",
    "    # Create the internal multi-layer cell for our RNN.\n",
    "    single_cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "    if use_lstm:\n",
    "      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n",
    "    cell = single_cell\n",
    "    if num_layers > 1:\n",
    "      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "    # The seq2seq function: we use embedding for the input and attention.\n",
    "    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "      return embedding_rnn_seq2seq(\n",
    "          encoder_inputs,\n",
    "          decoder_inputs,\n",
    "          cell,\n",
    "          num_encoder_symbols=source_vocab_size,\n",
    "          num_decoder_symbols=target_vocab_size,\n",
    "          embedding_size=size,\n",
    "          output_projection=output_projection,\n",
    "          feed_previous=do_decode,\n",
    "          dtype=dtype)\n",
    "\n",
    "    # Feeds for inputs.\n",
    "    self.encoder_inputs = []\n",
    "    self.decoder_inputs = []\n",
    "    self.target_weights = []\n",
    "    for i in range(buckets[-1][0]):  # Last bucket is the biggest one.\n",
    "      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"encoder{0}\".format(i)))\n",
    "    for i in range(buckets[-1][1] + 1):\n",
    "      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"decoder{0}\".format(i)))\n",
    "      self.target_weights.append(tf.placeholder(dtype, shape=[None],\n",
    "                                                name=\"weight{0}\".format(i)))\n",
    "\n",
    "    # Our targets are decoder inputs shifted by one.\n",
    "    targets = [self.decoder_inputs[i + 1]\n",
    "               for i in range(len(self.decoder_inputs) - 1)]\n",
    "    \n",
    "    # Training outputs and losses.\n",
    "    if forward_only:\n",
    "      self.outputs, self.losses = model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n",
    "          softmax_loss_function=softmax_loss_function)\n",
    "      # If we use output projection, we need to project outputs for decoding.\n",
    "      if output_projection is not None:\n",
    "        for b in range(len(buckets)):\n",
    "          self.outputs[b] = [\n",
    "              tf.matmul(output, output_projection[0]) + output_projection[1]\n",
    "              for output in self.outputs[b]\n",
    "          ]\n",
    "    else:\n",
    "      self.outputs, self.losses = model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets,\n",
    "          lambda x, y: seq2seq_f(x, y, False),\n",
    "          softmax_loss_function=softmax_loss_function)\n",
    "\n",
    "    # Gradients and SGD update operation for training the model.\n",
    "    params = tf.trainable_variables()\n",
    "    if not forward_only:\n",
    "      self.gradient_norms = []\n",
    "      self.updates = []\n",
    "      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "      for b in range(len(buckets)):\n",
    "        gradients = tf.gradients(self.losses[b], params)\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
    "                                                         max_gradient_norm)\n",
    "        self.gradient_norms.append(norm)\n",
    "        self.updates.append(opt.apply_gradients(\n",
    "            zip(clipped_gradients, params), global_step=self.global_step))\n",
    "\n",
    "    self.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
    "           bucket_id, forward_only):\n",
    "\n",
    "    # encoder and decoder sizes must match the corresponding bucket\n",
    "\n",
    "    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "    input_feed = {}\n",
    "    for l in range(encoder_size):\n",
    "      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "    for l in range(decoder_size):\n",
    "      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "      input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "    # Since our targets are decoder inputs shifted by one, we need one more.\n",
    "    last_target = self.decoder_inputs[decoder_size].name\n",
    "    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "\n",
    "    # Output feed: depends on whether we do a backward step or not.\n",
    "    if not forward_only:\n",
    "      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
    "                     self.gradient_norms[bucket_id],  # Gradient norm.\n",
    "                     self.losses[bucket_id]\n",
    "                    ]  \n",
    "    else:\n",
    "      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n",
    "      \n",
    "      for l in range(decoder_size):  # Output logits.\n",
    "        output_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "    outputs = session.run(output_feed, input_feed)\n",
    "    if not forward_only:\n",
    "      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
    "    else:\n",
    "      return None, outputs[0], outputs[1:-2]   # No gradient norm, loss, outputs.\n",
    "\n",
    "    \n",
    "  def get_batch(self, data, bucket_id):\n",
    "\n",
    "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "    encoder_inputs, decoder_inputs = [], []\n",
    "\n",
    "    # Get a random batch of encoder and decoder inputs from data,\n",
    "    # pad them if needed, reverse encoder inputs and add GO to decoder.\n",
    "    for _ in range(self.batch_size):\n",
    "      encoder_input, decoder_input = random.choice(data[bucket_id])\n",
    "\n",
    "      # Encoder inputs are padded and then reversed.\n",
    "      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n",
    "      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "\n",
    "      # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
    "      decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
    "      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n",
    "                            [data_utils.PAD_ID] * decoder_pad_size)\n",
    "\n",
    "    # Now we create batch-major vectors from the data selected above.\n",
    "    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "\n",
    "    # Batch encoder inputs are just re-indexed encoder_inputs.\n",
    "    for length_idx in range(encoder_size):\n",
    "      batch_encoder_inputs.append(\n",
    "          np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in range(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
    "    for length_idx in range(decoder_size):\n",
    "      batch_decoder_inputs.append(\n",
    "          np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in range(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "      # Create target_weights to be 0 for targets that are padding.\n",
    "      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
    "      for batch_idx in range(self.batch_size):\n",
    "        # We set weight to 0 if the corresponding target is a PAD symbol.\n",
    "        # The corresponding target is decoder_input shifted by 1 forward.\n",
    "        if length_idx < decoder_size - 1:\n",
    "          target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n",
    "          batch_weight[batch_idx] = 0.0\n",
    "      batch_weights.append(batch_weight)\n",
    "    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n",
    "  \n",
    "def make_dir(directory):\n",
    "  if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "  return directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing WMT data in seq2seq_data/\n",
      "Creating 2 layers of 512 units.\n",
      "Created model with fresh parameters.\n",
      "Reading development and training data (limit: 100000).\n",
      "  reading data line 100000\n",
      "logging summaries and errors in: seq2seq_train/\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import data_utils\n",
    "\n",
    "learning_rate = 0.5\n",
    "learning_rate_decay_factor = 0.99\n",
    "max_gradient_norm = 5.0\n",
    "batch_size = 64\n",
    "size = 512 # both the embedding size and the RNN state size\n",
    "num_layers = 2\n",
    "en_vocab_size = 40000\n",
    "fr_vocab_size = 40000\n",
    "data_dir = make_dir(\"seq2seq_data/\")\n",
    "train_dir = make_dir(\"seq2seq_train/\")\n",
    "max_train_data_size = 100000\n",
    "steps_per_checkpoint = 2000\n",
    "do_decode = False\n",
    "do_self_test = False\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See Seq2SeqModel for details of how they work.\n",
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "\n",
    "def read_data(source_path, target_path, max_size=None):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "  data_set = [[] for _ in _buckets]\n",
    "  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
    "    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
    "      source, target = source_file.readline(), target_file.readline()\n",
    "      counter = 0\n",
    "      while source and target and (not max_size or counter < max_size):\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  reading data line %d\" % counter)\n",
    "          sys.stdout.flush()\n",
    "        source_ids = [int(x) for x in source.split()]\n",
    "        target_ids = [int(x) for x in target.split()]\n",
    "        target_ids.append(data_utils.EOS_ID)\n",
    "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "            data_set[bucket_id].append([source_ids, target_ids])\n",
    "            break\n",
    "        source, target = source_file.readline(), target_file.readline()\n",
    "  return data_set\n",
    "\n",
    "\n",
    "def create_model(session, forward_only):\n",
    "  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "  model = Seq2SeqModel(\n",
    "      en_vocab_size,\n",
    "      fr_vocab_size,\n",
    "      _buckets,\n",
    "      size,\n",
    "      num_layers,\n",
    "      max_gradient_norm,\n",
    "      batch_size,\n",
    "      learning_rate,\n",
    "      learning_rate_decay_factor,\n",
    "      forward_only=forward_only)\n",
    "\n",
    "  ckpt = tf.train.get_checkpoint_state(train_dir)\n",
    "  if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "  return model\n",
    "\n",
    "def self_test():\n",
    "  \"\"\"Test the translation model.\"\"\"\n",
    "  with tf.Session() as sess: # make new session (no loading of saved session)\n",
    "    print(\"Self-test for neural translation model.\")\n",
    "    # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "    model = Seq2SeqModel(\n",
    "        10, # source vocab size\n",
    "        10, # target vocab size\n",
    "        [(3, 3),\n",
    "         (6, 6)], # buckets (max_en, max_fr)\n",
    "        1024, # size\n",
    "        2, # num layers\n",
    "        5.0, # max gradient norm\n",
    "        32, # batch size\n",
    "        0.3, # learning rate\n",
    "        0.99, # learning rate decay factor\n",
    "        use_lstm=False,\n",
    "        num_samples=8, # number of samples for sampled softmax\n",
    "        forward_only=False, # run 1-directional rnn (or something like that)\n",
    "        )\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "                [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "    for _ in range(10):  # Train the fake model for 5 steps.\n",
    "      bucket_id = random.choice([0, 1]) # choose a bucket\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "      _, step_loss, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "      print(\"step_loss: {}\".format(step_loss))\n",
    "      \n",
    "def write_batch_input_output_prediction_to_file(train_dir, bucket_num, global_step):\n",
    "  s = \"\"\n",
    "  # inputs\n",
    "  enc_inputs_npa = np.array([list(ar) for ar in encoder_inputs]).T\n",
    "  for sent_id in range(enc_inputs_npa.shape[0]):\n",
    "    sent = enc_inputs_npa[sent_id]\n",
    "    s += \"  \" + str(sent_id) + \": \" + \\\n",
    "         \" \".join(reversed([rev_en_vocab[n].decode(\"utf8\") for n in sent if n != 0])) + \"\\n\"\n",
    "\n",
    "  # outputs\n",
    "  decoder_inputs_npa = np.array([list(ar) for ar in decoder_inputs]).T\n",
    "  for sent_id in range(decoder_inputs_npa.shape[0]):\n",
    "    sent = decoder_inputs_npa[sent_id]\n",
    "    s += \"  \" + str(sent_id) + \": \" + \\\n",
    "         \" \".join([rev_fr_vocab[n].decode(\"utf8\") for n in sent if n != 0]) + \"\\n\"\n",
    "\n",
    "  # predicted outputs\n",
    "  pred_outputs = [list(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "  pred_outputs = np.array(pred_outputs).T\n",
    "  for sent_id in range(pred_outputs.shape[0]):\n",
    "    sent = pred_outputs[sent_id]\n",
    "    s += \"  \" + str(sent_id) + \": \" + \\\n",
    "         \" \".join([rev_fr_vocab[n].decode(\"utf8\") for n in sent if n != 0]) + \"\\n\"\n",
    "\n",
    "  with open(train_dir + \"batch_trans_\" + str(int(g_step)) + \\\n",
    "       \"_bucket_\" + str(bucket_id) + \".txt\", \"w\") as f:\n",
    "    f.write(s)\n",
    "\n",
    "if do_self_test:\n",
    "  self_test()\n",
    "else:\n",
    "  sess = tf.Session()\n",
    "  \"\"\"Train a en->fr translation model using WMT data.\"\"\"\n",
    "  # Prepare WMT data.\n",
    "  print(\"Preparing WMT data in %s\" % data_dir)\n",
    "  en_train, fr_train, en_dev, fr_dev, _, _ = data_utils.prepare_wmt_data(\n",
    "      data_dir, en_vocab_size, fr_vocab_size)\n",
    "\n",
    "  # Load vocabularies\n",
    "  en_vocab_path = os.path.join(data_dir, \"vocab%d.en\" % en_vocab_size)\n",
    "  fr_vocab_path = os.path.join(data_dir, \"vocab%d.fr\" % fr_vocab_size)\n",
    "  en_vocab, rev_en_vocab = data_utils.initialize_vocabulary(en_vocab_path)\n",
    "  fr_vocab, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)\n",
    "\n",
    "  sess = tf.Session()\n",
    "  # Create model.\n",
    "  print(\"Creating %d layers of %d units.\" % (num_layers, size))\n",
    "  model = create_model(sess, False)\n",
    "\n",
    "  # Read data into buckets and compute their sizes.\n",
    "  print (\"Reading development and training data (limit: %d).\"\n",
    "         % max_train_data_size)\n",
    "  dev_set = read_data(en_dev, fr_dev)\n",
    "  train_set = read_data(en_train, fr_train, max_train_data_size)\n",
    "  train_bucket_sizes = [len(train_set[b]) for b in range(len(_buckets))]\n",
    "  train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "  # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "  # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "  # the size if i-th training bucket, as used later.\n",
    "  train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                         for i in range(len(train_bucket_sizes))]\n",
    "\n",
    "  # This is the training loop.\n",
    "  step_time, loss = 0.0, 0.0\n",
    "  current_step = 0\n",
    "  previous_losses = []\n",
    "  while True:\n",
    "    # Choose a bucket according to data distribution. We pick a random number\n",
    "    # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "    random_number_01 = np.random.random_sample()\n",
    "    bucket_id = min([i for i in range(len(train_buckets_scale))\n",
    "                     if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "    # Get a batch and make a step.\n",
    "    start_time = time.time()\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "        train_set, bucket_id)\n",
    "    _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                 target_weights, bucket_id, False)\n",
    "    step_time += (time.time() - start_time) / steps_per_checkpoint\n",
    "    loss += step_loss / steps_per_checkpoint\n",
    "    current_step += 1\n",
    "\n",
    "    # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "    if True: # current_step % steps_per_checkpoint == 0: # --------------------------------------\n",
    "      print(\"logging summaries and errors in: \" + train_dir)\n",
    "      # Print statistics for the previous epoch.\n",
    "      perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "      g_step = sess.run(model.global_step)\n",
    "      l_rate = sess.run(model.learning_rate)\n",
    "      with open(train_dir + \"error_train_log.txt\", \"a\") as f:\n",
    "        f.write(\"train {} {} {} {}\\n\".format(g_step, l_rate, step_time, perplexity))\n",
    "      # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "      if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "        sess.run(model.learning_rate_decay_op)\n",
    "      previous_losses.append(loss)\n",
    "      # Save checkpoint and zero timer and loss.\n",
    "      checkpoint_path = os.path.join(train_dir, \"translate.ckpt\")\n",
    "#       model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "      step_time, loss = 0.0, 0.0\n",
    "      # Run evals on development set and print their perplexity. --------------------------\n",
    "      for bucket_id in range(len(_buckets)):\n",
    "        if len(dev_set[bucket_id]) == 0:\n",
    "          with open(train_dir + \"error_log.txt\", \"a\") as f:\n",
    "            f.write(\"test {} empty\\n\".format(bucket_id))\n",
    "          continue\n",
    "        encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id)\n",
    "        _, eval_loss, output_logits = model.step(sess, encoder_inputs,\n",
    "                                                 decoder_inputs, target_weights, bucket_id, True)\n",
    "        eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n",
    "        with open(train_dir + \"error_test_log.txt\", \"a\") as f:\n",
    "          f.write(\"test {} {}\\n\".format(bucket_id, eval_ppx))\n",
    "        \n",
    "        # print log of latent space (over the test batch)\n",
    "#         with open(train_dir + \"latent_zs.txt\", \"a\") as f:\n",
    "#           f.write(\"{} {}\\n\".format(bucket_id, lat_z))\n",
    "          \n",
    "        # print text example translations\n",
    "        write_batch_input_output_prediction_to_file(train_dir, bucket_id, g_step)\n",
    "        \n",
    "      # print out encoder embeddings as pandas dframe --------------------------------------\n",
    "      for var in tf.trainable_variables():\n",
    "        if var.name == 'embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding:0':\n",
    "            input_embedd_op = var\n",
    "      input_embedd_ar = sess.run(input_embedd_op)\n",
    "      df = pd.DataFrame(data=input_embedd_ar, index=[\n",
    "                rev_en_vocab[i].decode(\"utf8\") for i in range(input_embedd_ar.shape[0])])\n",
    "      df.head().to_csv(train_dir + \"embedd_df_\" + str(int(g_step)) + \".csv\")\n",
    "      # df.to_csv(train_dir + \"embedd_df_\" + str(int(g_step)) + \".csv\")\n",
    "      \n",
    "      # -------------------------------------------------------------------------------------\n",
    "\n",
    "      # break to capture environment for debug\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
