{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take some RNN models from tensorflow seq2seq:\n",
    "\n",
    "NOTE: much of this code was taken and modified from tensorflows github repository. Also this code is only compatible with an old version of tensorflow (0.10)\n",
    "\n",
    "- [x] For test data, show input batch words, output words, and predicted words (and print this to a file every so often)\n",
    "\n",
    "- [x] Switch to `embedding_rnn_seq2seq` model\n",
    "\n",
    "- [x] Capture word embeddings\n",
    "\n",
    "- [x] Normalize word embeddings\n",
    "\n",
    "- [x] Implement VAE\n",
    "\n",
    "- [x] Capture z's and latent loss\n",
    "\n",
    "- Minimize latent loss (hopefully this helps prevent exploding gradients !!)\n",
    "\n",
    "- [x] Get good summaries of Error, etc written to files during training\n",
    "\n",
    "- Any more finishing of model using original en -> fr data. Such as saving the tensorboard graph model.\n",
    "\n",
    "Figures:\n",
    "\n",
    "1. model diagram\n",
    "2. Training curves\n",
    "3. token embeddings\n",
    "4. visualize latent (color by sequence families or GO category)\n",
    "5. reconstruction (multiple seq alignment)\n",
    "\n",
    "## Now simply apply to protein data\n",
    "\n",
    "- Run protein data through the system using same tokenization machinery as for en -> fr translation\n",
    "\n",
    "- Have to change bucket sizes appropriatly\n",
    "\n",
    "- Tokenize 2 amino acids to get 400 total combos because easier to analyze embeddings. See if order matters ie does XY = YX ?\n",
    "\n",
    "- Color latent space by sequence attributes\n",
    "\n",
    "## Exploding Gradients are scary, no joke!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import rnn, rnn_cell, variable_scope, array_ops\n",
    "from tensorflow.python.ops.seq2seq import sequence_loss, embedding_rnn_decoder, embedding_attention_decoder\n",
    "\n",
    "# import data_utils\n",
    "import data_utils\n",
    "from data_utils import create_vocabulary, initialize_vocabulary, data_to_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_vae(x):\n",
    "  \"\"\" Takes a numeric tensor and returns the reconstructed mean tensor of same dims\n",
    "  (batch_size, input_dim)\"\"\"\n",
    "\n",
    "  # A lot of this code section is taken and modified from https://jmetzen.github.io/2015-11-27/vae.html\n",
    "\n",
    "  # hyper params:\n",
    "  n_input=size*2\n",
    "\n",
    "  # CREATE NETWORK\n",
    "\n",
    "  # 2) weights and biases variables\n",
    "  def xavier_init(fan_in, fan_out, constant=1): \n",
    "      \"\"\" Xavier initialization of network weights\"\"\"\n",
    "      # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "      low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "      high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "      return tf.random_uniform(( fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "\n",
    "  w_sigma = tf.Variable(xavier_init(n_input, n_z))\n",
    "  w_mean = tf.Variable(xavier_init(n_input, n_z))\n",
    "  w_reconstr = tf.Variable(xavier_init(n_z, n_input))\n",
    "  \n",
    "  b_sigma = tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "  b_mean = tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "  b_reconstr = tf.Variable(tf.zeros([n_input], dtype=tf.float32))\n",
    "\n",
    "  # 3) recognition network\n",
    "\n",
    "  # use recognition network to predict mean and (log) variance of (latent) Gaussian distribution z (n_z dimensional)\n",
    "  z_mean = tf.add(tf.matmul(x, w_mean), b_mean)\n",
    "  z_sigma = tf.add(tf.matmul(x, w_sigma), b_sigma)\n",
    "\n",
    "  # 4) do sampling on recognition network to get latent variables\n",
    "  # draw one n_z dimensional sample (for each input in batch), from normal distribution\n",
    "  eps = tf.random_normal((batch_size, n_z), 0, 1, dtype=tf.float32)\n",
    "\n",
    "  # scale that set of samples by predicted mu and epsilon to get samples of z, the latent distribution\n",
    "  # z = mu + sigma*epsilon\n",
    "  latent_z = tf.add(z_mean, tf.mul(tf.sqrt(tf.exp(z_sigma)), eps), name=\"latent_z\")\n",
    "\n",
    "  # 5) use generator network to predict mean of Bernoulli distribution of reconstructed input\n",
    "  x_reconstr_mean = tf.nn.sigmoid(tf.add(tf.matmul(latent_z, w_reconstr), b_reconstr))\n",
    "\n",
    "  # 6) loss\n",
    "  latent_loss = tf.mul(-0.5, tf.reduce_sum(1 + z_sigma - tf.square(z_mean) - tf.exp(z_sigma), 1), name=\"latent_loss\")\n",
    "\n",
    "  return x_reconstr_mean, latent_z, latent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model components from tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embedding_attention_seq2seq(encoder_inputs, decoder_inputs, cell,\n",
    "                                num_heads=1, output_projection=None,\n",
    "                                feed_previous=False, dtype=None, scope=None,\n",
    "                                initial_state_attention=False):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "\n",
    "  with variable_scope.variable_scope(\n",
    "      scope or \"embedding_attention_seq2seq\", dtype=dtype) as scope:\n",
    "    dtype = scope.dtype\n",
    "    # Encoder.\n",
    "    encoder_cell = rnn_cell.EmbeddingWrapper(\n",
    "        cell, embedding_classes=source_vocab_size,\n",
    "        embedding_size=size)\n",
    "    encoder_outputs, encoder_state = rnn.rnn(\n",
    "        encoder_cell, encoder_inputs, dtype=dtype)\n",
    "    \n",
    "    # Patrick: keep your eyes on that encoder state\n",
    "    encoder_state, latent_z, latent_loss = make_vae(encoder_state)\n",
    "\n",
    "    # First calculate a concatenation of encoder outputs to put attention on.\n",
    "    top_states = [array_ops.reshape(e, [-1, 1, cell.output_size])\n",
    "                  for e in encoder_outputs]\n",
    "    attention_states = array_ops.concat(1, top_states)\n",
    "\n",
    "    # Decoder.\n",
    "    output_size = None\n",
    "    if output_projection is None:\n",
    "      cell = rnn_cell.OutputProjectionWrapper(cell, target_vocab_size)\n",
    "      output_size = target_vocab_size\n",
    "\n",
    "    if isinstance(feed_previous, bool):\n",
    "      outputs, state = embedding_attention_decoder(\n",
    "          decoder_inputs,\n",
    "          encoder_state,\n",
    "          attention_states,\n",
    "          cell,\n",
    "          target_vocab_size,\n",
    "          size,\n",
    "          num_heads=num_heads,\n",
    "          output_size=output_size,\n",
    "          output_projection=output_projection,\n",
    "          feed_previous=feed_previous,\n",
    "          initial_state_attention=initial_state_attention)\n",
    "      return outputs, state, latent_z, latent_loss\n",
    "\n",
    "    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n",
    "    def decoder(feed_previous_bool):\n",
    "      reuse = None if feed_previous_bool else True\n",
    "      with variable_scope.variable_scope(\n",
    "          variable_scope.get_variable_scope(), reuse=reuse) as scope:\n",
    "        outputs, state = embedding_attention_decoder(\n",
    "            decoder_inputs,\n",
    "            encoder_state,\n",
    "            attention_states,\n",
    "            cell,\n",
    "            target_vocab_size,\n",
    "            size,\n",
    "            num_heads=num_heads,\n",
    "            output_size=output_size,\n",
    "            output_projection=output_projection,\n",
    "            feed_previous=feed_previous_bool,\n",
    "            update_embedding_for_previous=False,\n",
    "            initial_state_attention=initial_state_attention)\n",
    "        state_list = [state]\n",
    "        if nest.is_sequence(state):\n",
    "          state_list = nest.flatten(state)\n",
    "        return outputs + state_list\n",
    "\n",
    "    outputs_and_state = control_flow_ops.cond(feed_previous,\n",
    "                                              lambda: decoder(True),\n",
    "                                              lambda: decoder(False))\n",
    "    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n",
    "    state_list = outputs_and_state[outputs_len:]\n",
    "    state = state_list[0]\n",
    "    if nest.is_sequence(encoder_state):\n",
    "      state = nest.pack_sequence_as(structure=encoder_state,\n",
    "                                    flat_sequence=state_list)\n",
    "  return outputs_and_state[:outputs_len], state, latent_z, latent_loss\n",
    "\n",
    "def embedding_rnn_seq2seq(encoder_inputs, decoder_inputs, cell,\n",
    "                          output_projection=None,\n",
    "                          feed_previous=False, dtype=dtypes.float32,\n",
    "                          scope=None,):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "  with variable_scope.variable_scope(scope or \"embedding_rnn_seq2seq\"):\n",
    "    # Encoder.\n",
    "    encoder_cell = rnn_cell.EmbeddingWrapper(\n",
    "        cell, embedding_classes=source_vocab_size,\n",
    "        embedding_size=size)\n",
    "    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n",
    "    \n",
    "    # Patrick: keep your eyes on that encoder state\n",
    "    encoder_state, latent_z, latent_loss = make_vae(encoder_state)\n",
    "\n",
    "    # Decoder.\n",
    "    if output_projection is None:\n",
    "      cell = rnn_cell.OutputProjectionWrapper(cell, target_vocab_size)\n",
    "\n",
    "    if isinstance(feed_previous, bool):\n",
    "      outputs, state = embedding_rnn_decoder(\n",
    "          decoder_inputs, encoder_state, cell, target_vocab_size,\n",
    "          size, output_projection=output_projection,\n",
    "          feed_previous=feed_previous)\n",
    "      return outputs, state, latent_z, latent_loss\n",
    "\n",
    "    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n",
    "    def decoder(feed_previous_bool):\n",
    "      reuse = None if feed_previous_bool else True\n",
    "      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                         reuse=reuse):\n",
    "        outputs, state = embedding_rnn_decoder(\n",
    "            decoder_inputs, encoder_state, cell, target_vocab_size,\n",
    "            size, output_projection=output_projection,\n",
    "            feed_previous=feed_previous_bool,\n",
    "            update_embedding_for_previous=False)\n",
    "        state_list = [state]\n",
    "        if nest.is_sequence(state):\n",
    "          state_list = nest.flatten(state)\n",
    "        return outputs + state_list\n",
    "\n",
    "    outputs_and_state = control_flow_ops.cond(feed_previous,\n",
    "                                              lambda: decoder(True),\n",
    "                                              lambda: decoder(False))\n",
    "    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n",
    "    state_list = outputs_and_state[outputs_len:]\n",
    "    state = state_list[0]\n",
    "    if nest.is_sequence(encoder_state):\n",
    "      state = nest.pack_sequence_as(structure=encoder_state, flat_sequence=state_list)\n",
    "    \n",
    "    return outputs_and_state[:outputs_len], state, latent_z, latent_loss\n",
    "  \n",
    "def model_with_buckets(encoder_inputs, decoder_inputs, targets, weights,\n",
    "                       buckets, feed_previous, cell, output_projection=None,\n",
    "                       softmax_loss_function=None, name=None):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "\n",
    "  all_inputs = encoder_inputs + decoder_inputs + targets + weights\n",
    "  losses = []\n",
    "  outputs = []\n",
    "  \n",
    "  latent_losses = []\n",
    "  latent_zs = []\n",
    "  \n",
    "  with ops.op_scope(all_inputs, name, \"model_with_buckets\"):\n",
    "    for j, bucket in enumerate(buckets):\n",
    "      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                         reuse=True if j > 0 else None):\n",
    "        bucket_outputs, _, latent_z, latent_loss = embedding_attention_seq2seq(\n",
    "            encoder_inputs[:bucket[0]],\n",
    "            decoder_inputs[:bucket[1]],\n",
    "            cell,\n",
    "            output_projection=output_projection,\n",
    "            feed_previous=feed_previous)\n",
    "        \n",
    "        outputs.append(bucket_outputs)\n",
    "        \n",
    "        latent_losses.append(tf.reduce_mean(latent_loss))\n",
    "        latent_zs.append(latent_z)\n",
    "        \n",
    "        losses.append(sequence_loss(\n",
    "            outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n",
    "            softmax_loss_function=softmax_loss_function))\n",
    "\n",
    "  return outputs, losses, latent_losses, latent_zs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The translation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "  def __init__(self,\n",
    "               buckets,\n",
    "               size,\n",
    "               num_layers,\n",
    "               max_gradient_norm,\n",
    "               batch_size,\n",
    "               learning_rate,\n",
    "               learning_rate_decay_factor,\n",
    "               use_lstm=False,\n",
    "               num_samples=512,\n",
    "               forward_only=False,\n",
    "               dtype=tf.float32):\n",
    "    \n",
    "    self.buckets = buckets\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = tf.Variable(\n",
    "        float(learning_rate), trainable=False, dtype=dtype)\n",
    "    self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "        self.learning_rate * learning_rate_decay_factor)\n",
    "    self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # If we use sampled softmax, we need an output projection.\n",
    "    output_projection = None\n",
    "    softmax_loss_function = None\n",
    "    # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "    if num_samples > 0 and num_samples < target_vocab_size:\n",
    "      w_t = tf.get_variable(\"proj_w\", [target_vocab_size, size], dtype=dtype)\n",
    "      w = tf.transpose(w_t)\n",
    "      b = tf.get_variable(\"proj_b\", [target_vocab_size], dtype=dtype)\n",
    "      output_projection = (w, b)\n",
    "\n",
    "      def sampled_loss(inputs, labels):\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "        # avoid numerical instabilities.\n",
    "        local_w_t = tf.cast(w_t, tf.float32)\n",
    "        local_b = tf.cast(b, tf.float32)\n",
    "        local_inputs = tf.cast(inputs, tf.float32)\n",
    "        return tf.cast(\n",
    "            tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n",
    "                                       num_samples, target_vocab_size),dtype)\n",
    "      \n",
    "      softmax_loss_function = sampled_loss\n",
    "\n",
    "    # Create the internal multi-layer cell for our RNN.\n",
    "    single_cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "    if use_lstm:\n",
    "      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n",
    "    cell = single_cell\n",
    "    if num_layers > 1:\n",
    "      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "    # Feeds for inputs.\n",
    "    self.encoder_inputs = []\n",
    "    self.decoder_inputs = []\n",
    "    self.target_weights = []\n",
    "    for i in range(buckets[-1][0]):  # Last bucket is the biggest one.\n",
    "      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"encoder{0}\".format(i)))\n",
    "    for i in range(buckets[-1][1] + 1):\n",
    "      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"decoder{0}\".format(i)))\n",
    "      self.target_weights.append(tf.placeholder(dtype, shape=[None],\n",
    "                                                name=\"weight{0}\".format(i)))\n",
    "\n",
    "    # Our targets are decoder inputs shifted by one.\n",
    "    targets = [self.decoder_inputs[i + 1] for i in range(len(self.decoder_inputs) - 1)]\n",
    "    \n",
    "    # Training outputs and losses.\n",
    "    if forward_only:\n",
    "      self.outputs, self.losses, self.latent_losses, self.latent_zs = model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets,\n",
    "          feed_previous=True,\n",
    "          cell=cell,\n",
    "          softmax_loss_function=softmax_loss_function,\n",
    "          output_projection=output_projection\n",
    "      )\n",
    "      # If we use output projection, we need to project outputs for decoding.\n",
    "      if output_projection is not None:\n",
    "        for b in range(len(buckets)):\n",
    "          self.outputs[b] = [\n",
    "              tf.matmul(output, output_projection[0]) + output_projection[1]\n",
    "              for output in self.outputs[b]\n",
    "          ]\n",
    "    else:\n",
    "      self.outputs, self.losses, self.latent_losses, self.latent_zs = model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets,\n",
    "          feed_previous=False,\n",
    "          cell=cell,\n",
    "          softmax_loss_function=softmax_loss_function,\n",
    "          output_projection=output_projection)\n",
    "\n",
    "    # Gradients and SGD update operation for training the model.\n",
    "    params = tf.trainable_variables()\n",
    "    if not forward_only:\n",
    "      self.gradient_norms = []\n",
    "      self.updates = []\n",
    "      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "      for b in range(len(buckets)):\n",
    "        gradients = tf.gradients(self.losses[b] + self.latent_losses[b], params)\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
    "                                                         max_gradient_norm)\n",
    "        self.gradient_norms.append(norm)\n",
    "        self.updates.append(opt.apply_gradients(\n",
    "            zip(clipped_gradients, params), global_step=self.global_step))\n",
    "\n",
    "    self.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
    "           bucket_id, forward_only):\n",
    "\n",
    "    # encoder and decoder sizes must match the corresponding bucket\n",
    "    encoder_size = len(encoder_inputs)\n",
    "    decoder_size = len(decoder_inputs)\n",
    "\n",
    "    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "    input_feed = {}\n",
    "    for l in range(encoder_size):\n",
    "      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "    for l in range(decoder_size):\n",
    "      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "      input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "    # Since our targets are decoder inputs shifted by one, we need one more.\n",
    "    last_target = self.decoder_inputs[decoder_size].name\n",
    "    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "\n",
    "    # Output feed: depends on whether we do a backward step or not.\n",
    "    if not forward_only:\n",
    "      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
    "                     self.gradient_norms[bucket_id],  # Gradient norm.\n",
    "                     self.losses[bucket_id],\n",
    "                     self.latent_losses[bucket_id],\n",
    "                     self.latent_zs[bucket_id]\n",
    "                    ]  \n",
    "    else:\n",
    "      output_feed = [self.losses[bucket_id],\n",
    "                     self.latent_losses[bucket_id],\n",
    "                     self.latent_zs[bucket_id]]  # Loss for this batch.\n",
    "      \n",
    "      for l in range(decoder_size):  # Output logits.\n",
    "        output_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "    outputs = session.run(output_feed, input_feed)\n",
    "    \n",
    "    # this seems like ugly code, my bad ;(\n",
    "    if not forward_only:\n",
    "      return outputs[1], outputs[2], None, outputs[3], outputs[4]\n",
    "    else:\n",
    "      return None, outputs[0], outputs[3:], outputs[1], outputs[2]\n",
    "\n",
    "    \n",
    "  def get_batch(self, data, bucket_id):\n",
    "\n",
    "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "    encoder_inputs, decoder_inputs = [], []\n",
    "\n",
    "    # Get a random batch of encoder and decoder inputs from data,\n",
    "    # pad them if needed, reverse encoder inputs and add GO to decoder.\n",
    "    for _ in range(self.batch_size):\n",
    "      encoder_input, decoder_input = random.choice(data[bucket_id])\n",
    "\n",
    "      # Encoder inputs are padded and then reversed.\n",
    "      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n",
    "      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "\n",
    "      # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
    "      decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
    "      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n",
    "                            [data_utils.PAD_ID] * decoder_pad_size)\n",
    "\n",
    "    # Now we create batch-major vectors from the data selected above.\n",
    "    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "\n",
    "    # Batch encoder inputs are just re-indexed encoder_inputs.\n",
    "    for length_idx in range(encoder_size):\n",
    "      batch_encoder_inputs.append(\n",
    "          np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in range(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
    "    for length_idx in range(decoder_size):\n",
    "      batch_decoder_inputs.append(\n",
    "          np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in range(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "      # Create target_weights to be 0 for targets that are padding.\n",
    "      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
    "      for batch_idx in range(self.batch_size):\n",
    "        # We set weight to 0 if the corresponding target is a PAD symbol.\n",
    "        # The corresponding target is decoder_input shifted by 1 forward.\n",
    "        if length_idx < decoder_size - 1:\n",
    "          target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n",
    "          batch_weight[batch_idx] = 0.0\n",
    "      batch_weights.append(batch_weight)\n",
    "    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n",
    "  \n",
    "def make_dir(directory):\n",
    "  if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "  return directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  reading data line 100000\n",
      "  reading data line 200000\n",
      "  reading data line 300000\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.5\n",
    "learning_rate_decay_factor = 0.99\n",
    "max_gradient_norm = 2.0 # control exploding gradients !!\n",
    "batch_size = 32\n",
    "size = 96 # both the embedding size and the RNN state size\n",
    "n_z = 24\n",
    "num_layers = 2\n",
    "source_vocab_size = 400\n",
    "target_vocab_size = 400\n",
    "train_dir = make_dir(\"seq2seq_train_protein_z\" + str(n_z) +\"/\")\n",
    "max_train_data_size = 6100000\n",
    "steps_per_checkpoint = 1500\n",
    "do_self_test = False\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See Seq2SeqModel for details of how they work.\n",
    "_buckets = [(10, 10), (20, 20), (30, 30), (50, 50)]\n",
    "\n",
    "def read_data(source_path, target_path, max_size=None):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "  data_set = [[] for _ in _buckets]\n",
    "  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
    "    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
    "      source, target = source_file.readline(), target_file.readline()\n",
    "      counter = 0\n",
    "      while source and target and (not max_size or counter < max_size):\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  reading data line %d\" % counter)\n",
    "          sys.stdout.flush()\n",
    "        source_ids = [int(x) for x in source.split()]\n",
    "        target_ids = [int(x) for x in target.split()]\n",
    "        target_ids.append(data_utils.EOS_ID)\n",
    "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "            data_set[bucket_id].append([source_ids, target_ids])\n",
    "            break\n",
    "        source, target = source_file.readline(), target_file.readline()\n",
    "  return data_set\n",
    "\n",
    "def create_model(session, forward_only):\n",
    "  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "  model = Seq2SeqModel(\n",
    "      _buckets,\n",
    "      size,\n",
    "      num_layers,\n",
    "      max_gradient_norm,\n",
    "      batch_size,\n",
    "      learning_rate,\n",
    "      learning_rate_decay_factor,\n",
    "      forward_only=forward_only)\n",
    "\n",
    "  ckpt = tf.train.get_checkpoint_state(train_dir)\n",
    "  if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "  return model\n",
    "      \n",
    "def write_batch_input_output_prediction_to_file(n_examples=5, n_sample=3):\n",
    "  # note_ n_examples must be less than or equal to the batch size\n",
    "  s = \"\"\n",
    "  input_seqs = []\n",
    "  enc_inputs_npa = np.array([list(ar) for ar in encoder_inputs]).T\n",
    "  \n",
    "  for sent_id in range(batch_size):\n",
    "    sent_in = enc_inputs_npa[sent_id]\n",
    "    str_input_seq = str(sent_id) + \": \" + \" \".join(\n",
    "      reversed([rev_vocab[n].decode(\"utf8\") for n in sent_in if n != 0])\n",
    "    ) + \"\\n\"\n",
    "    input_seqs.append(str_input_seq)\n",
    "    \n",
    "  for sent_id in range(n_examples):\n",
    "    # print n_examples of \"real\" inputs\n",
    "    sent_in = enc_inputs_npa[sent_id]\n",
    "    s += input_seqs[sent_id]\n",
    "    \n",
    "    # for each \"real\" input, print n_sample samples\n",
    "    for sample_id in range(n_sample):\n",
    "      _, eval_loss, output_logits, latent_loss, latent_z = model.step(sess, encoder_inputs,\n",
    "                                                 decoder_inputs, target_weights, bucket_id, True)\n",
    "      pred_outputs = np.array([list(np.argmax(logit, axis=1)) for logit in output_logits]).T\n",
    "      s += str(sent_id) + \": \" + \" \".join([rev_vocab[n].decode(\"utf8\") for n in pred_outputs[sent_id] if n != 0]) + \"\\n\"\n",
    "\n",
    "\n",
    "  with open(train_dir + \"batch_trans_\" + str(int(g_step)) + \"_bucket_\" + str(bucket_id) + \".txt\", \"w\") as f:\n",
    "    f.write(s)\n",
    "    \n",
    "  # print log of latent space (over the test batch)\n",
    "  df = pd.DataFrame(latent_z, index=input_seqs)\n",
    "  df.to_csv(train_dir + \"latent_z_{}_{}.txt\".format(bucket_id, g_step), sep=\" \")\n",
    "#   np.savetxt(train_dir + \"latent_z_{}_{}.txt\".format(bucket_id, g_step), latent_z, delimiter=\" \")\n",
    "\n",
    "if do_self_test:\n",
    "  source_vocab_size = 10\n",
    "  target_vocab_size = 10\n",
    "  \n",
    "  \"\"\"Test the translation model.\"\"\"\n",
    "  sess = tf.Session() # make new session (no loading of saved session)\n",
    "  print(\"Self-test for neural translation model.\")\n",
    "  # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "  model = Seq2SeqModel(\n",
    "      [(3, 3),(6, 6)], # buckets (max_en, max_fr)\n",
    "      512, # size\n",
    "      2, # num layers\n",
    "      5.0, # max gradient norm\n",
    "      batch_size, # batch size\n",
    "      0.3, # learning rate\n",
    "      0.99, # learning rate decay factor\n",
    "      use_lstm=False,\n",
    "      num_samples=8, # number of samples for sampled softmax\n",
    "      forward_only=False, # run 1-directional rnn (or something like that)\n",
    "      )\n",
    "  sess.run(tf.initialize_all_variables())\n",
    "\n",
    "  data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "              [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "  for _ in range(10):  # Train the fake model for 5 steps.\n",
    "    bucket_id = random.choice([0, 1]) # choose a bucket\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "    _, step_loss, output_logits, latent_loss, latent_z = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "    print(\"step_loss: {}, latent_loss: {}, latent_z: {}\".format(step_loss, latent_loss, latent_z))\n",
    "\n",
    "else:\n",
    "  sess = tf.Session()\n",
    "\n",
    "  def init_vocabs():\n",
    "    data_file = \"protein_data/protein.txt\"\n",
    "    dev_data_file = \"protein_data/protein_dev.txt\"\n",
    "    vocab_file = \"protein_data/vocab.txt\"\n",
    "\n",
    "    dev_token_file = \"protein_data/dev_token.txt\"\n",
    "    data_token_file = \"protein_data/token.txt\"\n",
    "\n",
    "    # use same data as input and output (autoencoder)\n",
    "    create_vocabulary(vocab_file, data_file, source_vocab_size, None)\n",
    "    data_to_token_ids(dev_data_file, dev_token_file, vocab_file, None)\n",
    "    data_to_token_ids(data_file, data_token_file, vocab_file, None)\n",
    "    \n",
    "    dev_set = read_data(dev_token_file, dev_token_file)\n",
    "    train_set = read_data(data_token_file, data_token_file, max_train_data_size)\n",
    "    vocab, rev_vocab = initialize_vocabulary(vocab_file)\n",
    "\n",
    "    return vocab, rev_vocab, dev_set, train_set\n",
    "\n",
    "  vocab, rev_vocab, dev_set, train_set = init_vocabs()\n",
    "  \n",
    "  sess = tf.Session()\n",
    "  # Create model.\n",
    "  print(\"Creating %d layers of %d units.\" % (num_layers, size))\n",
    "  model = create_model(sess, False)\n",
    "\n",
    "  # Read data into buckets and compute their sizes.\n",
    "  print (\"Reading development and training data (limit: %d).\"\n",
    "         % max_train_data_size)\n",
    "#   dev_set = read_data(en_dev, fr_dev)\n",
    "#   train_set = read_data(en_train, fr_train, max_train_data_size)\n",
    "  train_bucket_sizes = [len(train_set[b]) for b in range(len(_buckets))]\n",
    "  train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "  # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "  # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "  # the size if i-th training bucket, as used later.\n",
    "  train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                         for i in range(len(train_bucket_sizes))]\n",
    "\n",
    "  # This is the training loop.\n",
    "  step_time, loss, l_loss = 0.0, 0.0, 0.0\n",
    "  current_step = 0\n",
    "  previous_losses = []\n",
    "  while True:\n",
    "    # Choose a bucket according to data distribution. We pick a random number\n",
    "    # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "    random_number_01 = np.random.random_sample()\n",
    "    bucket_id = min([i for i in range(len(train_buckets_scale))\n",
    "                     if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "    # Get a batch and make a step.\n",
    "    start_time = time.time()\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "        train_set, bucket_id)\n",
    "    _, step_loss, _, latent_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                 target_weights, bucket_id, False)\n",
    "    step_time += (time.time() - start_time) / steps_per_checkpoint\n",
    "    loss += step_loss / steps_per_checkpoint\n",
    "    l_loss += latent_loss / steps_per_checkpoint\n",
    "    current_step += 1\n",
    "\n",
    "    # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "    if current_step % steps_per_checkpoint == 0: # --------------------------------------\n",
    "      print(\"logging summaries and errors in: \" + train_dir)\n",
    "      # Print statistics for the previous epoch.\n",
    "      perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "      g_step = sess.run(model.global_step)\n",
    "      l_rate = sess.run(model.learning_rate)\n",
    "      with open(train_dir + \"error_train_log.txt\", \"a\") as f:\n",
    "        f.write(\"train {} {} {}\\n\".format(l_rate, loss, l_loss))\n",
    "      # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "      if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "        sess.run(model.learning_rate_decay_op)\n",
    "      previous_losses.append(loss)\n",
    "      # Save checkpoint and zero timer and loss.\n",
    "      checkpoint_path = os.path.join(train_dir, \"translate.ckpt\")\n",
    "      model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "      step_time, loss, l_loss = 0.0, 0.0, 0.0\n",
    "      # Run evals on development set and print their perplexity. --------------------------\n",
    "      for bucket_id in range(len(_buckets)):\n",
    "        if len(dev_set[bucket_id]) == 0:\n",
    "          with open(train_dir + \"error_log.txt\", \"a\") as f:\n",
    "            f.write(\"test {} empty empty\\n\".format(bucket_id))\n",
    "          continue\n",
    "        encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id)\n",
    "        _, eval_loss, output_logits, latent_loss, latent_z = model.step(sess, encoder_inputs,\n",
    "                                                 decoder_inputs, target_weights, bucket_id, True)\n",
    "        eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n",
    "        with open(train_dir + \"error_test_log.txt\", \"a\") as f:\n",
    "          f.write(\"test {} {} {}\\n\".format(bucket_id, eval_loss, latent_loss))\n",
    "        \n",
    "\n",
    "          \n",
    "        # print text example translations (note: this should be last thing in test because we are going to do stuff)\n",
    "        write_batch_input_output_prediction_to_file()\n",
    "        \n",
    "      # print out encoder embeddings as pandas dframe --------------------------------------\n",
    "      for var in tf.trainable_variables():\n",
    "        if var.name == 'embedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding:0':\n",
    "          input_embedd_op = var\n",
    "      \n",
    "      norm = tf.sqrt(tf.reduce_sum(tf.square(input_embedd_op), 1, keep_dims=True))\n",
    "      normalized_embeddings = input_embedd_op / norm\n",
    "      input_embedd_ar = sess.run(normalized_embeddings)\n",
    "      \n",
    "      token_strs = [rev_vocab[i].decode(\"utf8\") for i in range(input_embedd_ar.shape[0])]\n",
    "      df = pd.DataFrame(data=input_embedd_ar, columns=list(range(input_embedd_ar.shape[1])), index=token_strs)\n",
    "      df.to_csv(train_dir + \"embedd_df_\" + str(int(g_step)) + \".csv\", sep=\" \")\n",
    "      \n",
    "      # -------------------------------------------------------------------------------------\n",
    "\n",
    "      # break to capture environment for debug\n",
    "#       break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# description of data\n",
    "\n",
    "# distribution of read lengths and how many in each bucket\n",
    "print(\"train bucket sizes: {}\".format(train_bucket_sizes))\n",
    "print(\"test bucket sizes: {}\".format(list([len(dev_set[i]) for i in range(len(dev_set))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Memory used: \" + str(process.memory_info().rss/1000000000) + \" GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for bucket_id in range(len(dev_set)):\n",
    "  bucket_len = []\n",
    "  for seq_id in range(len(dev_set[bucket_id])):\n",
    "    bucket_len.append(len(dev_set[bucket_id][seq_id][0]))\n",
    "  plt.hist(np.array(bucket_len))\n",
    "  plt.title(\"Testing Set Peptide Lengths\")\n",
    "  plt.xlabel(\"Length\")\n",
    "  plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "  \n",
    "for bucket_id in range(len(train_set)):\n",
    "  bucket_len = []\n",
    "  for seq_id in range(len(train_set[bucket_id])):\n",
    "    bucket_len.append(len(train_set[bucket_id][seq_id][0]))\n",
    "  plt.hist(np.array(bucket_len))\n",
    "  plt.title(\"Training Set Peptide Lengths\")\n",
    "  plt.xlabel(\"Length\")\n",
    "  plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# def xavier_init(fan_in, fan_out, constant=1): \n",
    "#     \"\"\" Xavier initialization of network weights\"\"\"\n",
    "#     # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "#     low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "#     high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "#     return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "  \n",
    "# n_z = 10\n",
    "# n_input = 5\n",
    "# batch_size = 16\n",
    "\n",
    "# x = tf.placeholder(tf.float32, shape=(batch_size, n_input))\n",
    "# # x = tf.Variable(xavier_init(batch_size, n_input))\n",
    "  \n",
    "# b_mean = tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "\n",
    "# w_mean = tf.Variable(xavier_init(n_input, n_z))\n",
    "  \n",
    "# z_mean = tf.add(tf.matmul(x, w_mean), b_mean)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# # make data\n",
    "# x_feed =  np.random.normal(loc=0.0, scale=1.0, size=(batch_size, n_input))\n",
    "# x_feed\n",
    "\n",
    "# # test out example graph\n",
    "# # sess.run(z_mean, feed_dict={x: x_feed}).shape\n",
    "# z_mean.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
