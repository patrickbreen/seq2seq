{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependancies, parameters, and setup\n",
    "# important paper do not forget:\n",
    "# https://arxiv.org/pdf/1610.02415v1.pdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "train_dir = \"/home/ubuntu/Desktop/seq2seq/seq2seq_train_protein/\"\n",
    "data_dir = \"/home/ubuntu/Desktop/seq2seq/protein_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# error training summaries\n",
    "\n",
    "# training error\n",
    "df_train_er = pd.read_csv(train_dir + \"error_train_log.txt\",\n",
    "                          names=[\"\", \"global step\", \"learning rate\", \"step time\", \"loss\", \"latent loss\"],\n",
    "                          sep=\" \",\n",
    "                          index_col=False\n",
    "                         )\n",
    "# step time\n",
    "# df_train_er.plot(y=3, title=\"train step time\")\n",
    "\n",
    "# loss\n",
    "# df_train_er.plot(y=4, title=\"train loss\")\n",
    "\n",
    "# latent_loss\n",
    "# df_train_er.plot(y=5, title=\"train latent loss\")\n",
    "\n",
    "# testing error by bucket\n",
    "df_test_er = pd.read_csv(train_dir + \"error_test_log.txt\",\n",
    "                          names=[\"\", \"bucket_id\", \"loss\", \"latent loss\"],\n",
    "                          sep=\" \",\n",
    "                          index_col=False\n",
    "                         )\n",
    "df_test_er.index = list(range(1500,(df_test_er.shape[0]+1)*1500, 1500))\n",
    "\n",
    "# df_test_er = df_test_er.iloc[:200,:]\n",
    "\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True, sharey=False)\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "for bucket_id in range(4):\n",
    "  ax = axes[bucket_id]\n",
    "  ax = df_test_er[df_test_er.bucket_id == bucket_id][[\"loss\", \"latent loss\"]].plot(\n",
    "    title=\"bucket id: \" + str(bucket_id), ax=ax, figsize=(8, 6))\n",
    "  ax.set_xlabel('number of batches', fontsize=10)\n",
    "  ax.set_ylabel('log perplexity', fontsize = 10)\n",
    "  ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "  ax.ticklabel_format(axis='both', style='sci', scilimits=(-4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize embeddings\n",
    "\n",
    "# embeddings after batch 1,500\n",
    "latent_df = pd.read_csv(train_dir + \"embedd_df_1.csv\", sep=\" \")\n",
    "ax = sns.clustermap(cosine_similarity(np.random.normal(size=latent_df.iloc[:,1:].values.shape)), figsize=(8, 6))\n",
    "\n",
    "# embeddings after batch 171,000\n",
    "latent_df = pd.read_csv(train_dir + \"embedd_df_31501.csv\", sep=\" \")\n",
    "ax = sns.clustermap(cosine_similarity(latent_df.iloc[:,1:].values), figsize=(8, 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize latent space distribution (by bucket)\n",
    "\n",
    "# latent_z = pd.read_csv(train_dir + \"latent_z_0_31501.txt\", sep=\" \", header=None)\n",
    "# model = TSNE(n_components=2, random_state=0)\n",
    "# np.set_printoptions(suppress=True)\n",
    "# x, y = model.fit_transform(latent_z.iloc[]).T\n",
    "# plt.scatter(x, y)\n",
    "# latent_z\n",
    "\n",
    "# TODO color scatter plot by hydrophobicity\n",
    "\n",
    "# TODO color scatter plot (binary) by transmembrane vs not transmembrane (requires lookup on NCBI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make plot of represenation colored by sequence length\n",
    "files = [\"latent_z_0_31501.txt\", \"latent_z_1_31501.txt\", \"latent_z_2_31501.txt\", \"latent_z_3_31501.txt\"]\n",
    "\n",
    "def get_lens_and_reps(filename):\n",
    "  lengths = []\n",
    "  z_representations = []\n",
    "  seqs = []\n",
    "\n",
    "  lines = open(train_dir + filename).readlines()\n",
    "  assert len(lines) % 2 == 0\n",
    "  n_lines = len(lines)\n",
    "\n",
    "  for i in range(0, n_lines, 2):\n",
    "    sequence = \"\".join(lines[i].split()[1:])\n",
    "    seqs.append(sequence)\n",
    "    representation = lines[i+1]\n",
    "\n",
    "    length = len(sequence)\n",
    "    lengths.append(length)\n",
    "    rep_formated = np.array([float(num) for num in representation.split()[1:]])\n",
    "    z_representations.append(rep_formated)\n",
    "\n",
    "  return lengths, z_representations, seqs\n",
    "\n",
    "lengths = []\n",
    "z_reps = []\n",
    "seqs = []\n",
    "\n",
    "for filename in files:\n",
    "  bucket_lengths, bucket_z_reps, bucket_seqs = get_lens_and_reps(filename)\n",
    "  lengths = lengths + bucket_lengths\n",
    "  z_reps = z_reps + bucket_z_reps\n",
    "  seqs = seqs + bucket_seqs\n",
    "\n",
    "data = np.array(z_reps)\n",
    "colors = np.array(lengths)\n",
    "\n",
    "# do tSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "x, y = model.fit_transform(data).T\n",
    "\n",
    "# do plot\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "points = ax.scatter(x, y, c=colors, s=30, cmap=plt.get_cmap(\"Spectral\"))\n",
    "f.colorbar(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now do the same, but now colored for hydrophobicity / seq_length\n",
    "\n",
    "kd = { 'A': 1.8,'R':-4.5,'N':-3.5,'D':-3.5,'C': 2.5,\n",
    "       'Q':-3.5,'E':-3.5,'G':-0.4,'H':-3.2,'I': 4.5,\n",
    "       'L': 3.8,'K':-3.9,'M': 1.9,'F': 2.8,'P':-1.6,\n",
    "       'S':-0.8,'T':-0.7,'W':-0.9,'Y':-1.3,'V': 4.2 }\n",
    "\n",
    "colors = []\n",
    "for seq in seqs:\n",
    "  seq = seq.replace(\"_UNK\", \"\")\n",
    "  color = sum([kd[aa] for aa in seq if aa in kd.keys()]) / len(seq)\n",
    "  colors.append(color)\n",
    "  \n",
    "colors = np.array(colors)\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "sort_ind = np.argsort(colors)\n",
    "\n",
    "points = ax.scatter(x[sort_ind], y[sort_ind], c=colors[sort_ind], s=30, cmap=plt.get_cmap(\"Spectral\"))\n",
    "f.colorbar(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now do the same, but now colored binary for transmembrane or not transmembrane\n",
    "\n",
    "headers = open(\"protein_data/protein_dev_headers.txt\").readlines()\n",
    "\n",
    "# make sure headers are ordered correctly (\"header seq\" should match \"seqs\" order)\n",
    "header_seqs = [head.strip()split(\" \")[-1] for head in headers]\n",
    "header_sorted_inds = [seqs.index(h_seq) for h_seq in header_seqs]\n",
    "\n",
    "# do sort of headers\n",
    "headers = [headers[i] for i in header_sorted_inds]\n",
    "\n",
    "colors = [\"membrane\" in header.lower() for header in headers]\n",
    "\n",
    "# color \"transmembrane as one color and the rest as another color\n",
    "# make sure transmembrane is plotted in the foreground\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "sort_ind = np.argsort(colors)\n",
    "\n",
    "points = ax.scatter(x[sort_ind], y[sort_ind], c=colors[sort_ind], s=30, cmap=plt.get_cmap(\"Spectral\"))\n",
    "f.colorbar(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO for a given peptide of interest, show sampling of z's and outputs with high alpha against low alpha testing set\n",
    "\n",
    "# TODO show unguided (fully markov) random walk for a few iterations in latent space and output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example translations\n",
    "for bucket_id in range(4):\n",
    "  print(\"Bucket \" + str(bucket_id) + \": \")\n",
    "  with open(train_dir + \"batch_trans_31501_bucket_{}.txt\".format(bucket_id)) as f_in:\n",
    "    for l in f_in:\n",
    "      tokens = l.strip().split(\" \")\n",
    "      last_tokens = [t for t in tokens[1:] if t not in [\"_EOS\", \"_GO\"]]\n",
    "      s = \"\".join(last_tokens).replace(\"_UNK\", \"_\")\n",
    "      print(tokens[0] + \" \" + s)\n",
    "  print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
