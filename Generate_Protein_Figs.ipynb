{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependancies, parameters, and setup\n",
    "# important paper do not forget:\n",
    "# https://arxiv.org/pdf/1610.02415v1.pdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "train_dir = \"/home/ubuntu/Desktop/seq2seq/seq2seq_train_protein/\"\n",
    "data_dir = \"/home/ubuntu/Desktop/seq2seq/protein_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# error training summaries\n",
    "\n",
    "# training error\n",
    "df_train_er = pd.read_csv(train_dir + \"error_train_log.txt\",\n",
    "                          names=[\"\", \"global step\", \"learning rate\", \"step time\", \"loss\", \"latent loss\"],\n",
    "                          sep=\" \",\n",
    "                          index_col=False\n",
    "                         )\n",
    "# step time\n",
    "df_train_er.plot(y=3, title=\"train step time\")\n",
    "\n",
    "# loss\n",
    "df_train_er.plot(y=4, title=\"train loss\")\n",
    "\n",
    "# latent_loss\n",
    "df_train_er.plot(y=5, title=\"train latent loss\")\n",
    "\n",
    "# testing error by bucket\n",
    "df_test_er = pd.read_csv(train_dir + \"error_test_log.txt\",\n",
    "                          names=[\"\", \"bucket_id\", \"loss\", \"latent loss\"],\n",
    "                          sep=\" \",\n",
    "                          index_col=False\n",
    "                         )\n",
    "\n",
    "for bucket_id in range(4):\n",
    "  df_test_er[df_test_er.bucket_id == bucket_id][[\"loss\", \"latent loss\"]].plot(title=\"bucket id: \" + str(bucket_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize embeddings\n",
    "latent_df = pd.read_csv(train_dir + \"embedd_df_124000.csv\", sep=\" \")\n",
    "sns.clustermap(cosine_similarity(latent_df.iloc[:,1:].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize latent space distribution (by bucket)\n",
    "latent_z = pd.read_csv(train_dir + \"latent_z_3_1.txt\", sep=\" \")\n",
    "latent_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example translations\n",
    "for bucket_id in range(4):\n",
    "  print(\"Bucket \" + str(bucket_id) + \": \")\n",
    "  with open(train_dir + \"batch_trans_50000_bucket_{}.txt\".format(bucket_id)) as f_in:\n",
    "    for l in f_in:\n",
    "      tokens = l.strip().split(\" \")\n",
    "      last_tokens = [t for t in tokens[1:] if t not in [\"_EOS\", \"_GO\"]]\n",
    "      s = \"\".join(last_tokens).replace(\"_UNK\", \"_\")\n",
    "      print(tokens[0] + \" \" + s)\n",
    "  print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
