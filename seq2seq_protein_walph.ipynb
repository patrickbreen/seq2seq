{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take some RNN models from tensorflow seq2seq:\n",
    "\n",
    "NOTE: much of this code was taken and modified from tensorflows github repository. Also this code is only compatible with an old version of tensorflow (0.10)\n",
    "\n",
    "- [x] For test data, show input batch words, output words, and predicted words (and print this to a file every so often)\n",
    "\n",
    "- [x] Switch to `embedding_rnn_seq2seq` model\n",
    "\n",
    "- [x] Capture word embeddings\n",
    "\n",
    "- [x] Normalize word embeddings\n",
    "\n",
    "- [x] Implement VAE\n",
    "\n",
    "- [x] Capture z's and latent loss\n",
    "\n",
    "- Minimize latent loss (hopefully this helps prevent exploding gradients !!)\n",
    "\n",
    "- [x] Get good summaries of Error, etc written to files during training\n",
    "\n",
    "- Any more finishing of model using original en -> fr data. Such as saving the tensorboard graph model.\n",
    "\n",
    "Figures:\n",
    "\n",
    "1. model diagram\n",
    "2. Training curves\n",
    "3. token embeddings\n",
    "4. visualize latent (color by sequence families or GO category)\n",
    "5. reconstruction (multiple seq alignment)\n",
    "\n",
    "## Now simply apply to protein data\n",
    "\n",
    "- Run protein data through the system using same tokenization machinery as for en -> fr translation\n",
    "\n",
    "- Have to change bucket sizes appropriatly\n",
    "\n",
    "- Tokenize 2 amino acids to get 400 total combos because easier to analyze embeddings. See if order matters ie does XY = YX ?\n",
    "\n",
    "- Color latent space by sequence attributes\n",
    "\n",
    "## Exploding Gradients are scary, no joke!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import rnn, rnn_cell, variable_scope, array_ops\n",
    "from tensorflow.python.ops.seq2seq import sequence_loss, embedding_rnn_decoder, embedding_attention_decoder\n",
    "\n",
    "# import data_utils\n",
    "import data_utils\n",
    "from data_utils import create_vocabulary, initialize_vocabulary, data_to_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_vae(x):\n",
    "  \"\"\" Takes a numeric tensor and returns the reconstructed mean tensor of same dims\n",
    "  (batch_size, input_dim)\"\"\"\n",
    "\n",
    "  # A lot of this code section is taken and modified from https://jmetzen.github.io/2015-11-27/vae.html\n",
    "\n",
    "  # hyper params:\n",
    "  n_input=size*2\n",
    "\n",
    "  # CREATE NETWORK\n",
    "\n",
    "  # 2) weights and biases variables\n",
    "  def xavier_init(fan_in, fan_out, constant=1): \n",
    "      \"\"\" Xavier initialization of network weights\"\"\"\n",
    "      # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "      low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "      high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "      return tf.random_uniform(shape=[fan_in, fan_out], minval=low, maxval=high, dtype=tf.float32)\n",
    "\n",
    "  w_sigma = tf.Variable(xavier_init(n_input, n_z))\n",
    "  w_mean = tf.Variable(xavier_init(n_input, n_z))\n",
    "  w_reconstr = tf.Variable(xavier_init(n_z, n_input))\n",
    "  \n",
    "  b_sigma = tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "  b_mean = tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "  b_reconstr = tf.Variable(tf.zeros([n_input], dtype=tf.float32))\n",
    "\n",
    "  # 3) recognition network\n",
    "\n",
    "  # use recognition network to predict mean and (log) variance of (latent) Gaussian distribution z (n_z dimensional)\n",
    "  z_mean = tf.add(tf.matmul(x, w_mean), b_mean)\n",
    "  z_sigma = tf.add(tf.matmul(x, w_sigma), b_sigma)\n",
    "\n",
    "  # 4) do sampling on recognition network to get latent variables\n",
    "  # draw one n_z dimensional sample (for each input in batch), from normal distribution\n",
    "  eps = tf.random_normal([n_z], 0, 1, dtype=tf.float32)\n",
    "\n",
    "  # scale that set of samples by predicted mu and epsilon to get samples of z, the latent distribution\n",
    "  # z = mu + sigma*epsilon\n",
    "  latent_z = tf.add(z_mean, tf.mul(tf.sqrt(tf.exp(z_sigma)), eps), name=\"latent_z\")\n",
    "\n",
    "  # 5) use generator network to predict mean of Bernoulli distribution of reconstructed input\n",
    "  x_reconstr_mean = tf.nn.sigmoid(tf.add(tf.matmul(latent_z, w_reconstr), b_reconstr))\n",
    "\n",
    "  # 6) loss\n",
    "  latent_loss = tf.mul(-0.5, tf.reduce_sum(1 + z_sigma - tf.square(z_mean) - tf.exp(z_sigma), 1), name=\"latent_loss\")\n",
    "\n",
    "  return x_reconstr_mean, latent_z, latent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model components from tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embedding_attention_seq2seq(encoder_inputs, decoder_inputs, cell,\n",
    "                                num_heads=1, output_projection=None,\n",
    "                                feed_previous=False, dtype=None, scope=None,\n",
    "                                initial_state_attention=False):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "\n",
    "  with variable_scope.variable_scope(\n",
    "      scope or \"embedding_attention_seq2seq\", dtype=dtype) as scope:\n",
    "    dtype = scope.dtype\n",
    "    # Encoder.\n",
    "    encoder_cell = rnn_cell.EmbeddingWrapper(\n",
    "        cell, embedding_classes=source_vocab_size,\n",
    "        embedding_size=size)\n",
    "    encoder_outputs, encoder_state = rnn.rnn(\n",
    "        encoder_cell, encoder_inputs, dtype=dtype)\n",
    "    \n",
    "    # Patrick: keep your eyes on that encoder state\n",
    "    encoder_state, latent_z, latent_loss = make_vae(encoder_state)\n",
    "\n",
    "    # First calculate a concatenation of encoder outputs to put attention on.\n",
    "    top_states = [array_ops.reshape(e, [-1, 1, cell.output_size])\n",
    "                  for e in encoder_outputs]\n",
    "    attention_states = array_ops.concat(1, top_states)\n",
    "\n",
    "    # Decoder.\n",
    "    output_size = None\n",
    "    if output_projection is None:\n",
    "      cell = rnn_cell.OutputProjectionWrapper(cell, target_vocab_size)\n",
    "      output_size = target_vocab_size\n",
    "\n",
    "    if isinstance(feed_previous, bool):\n",
    "      outputs, state = embedding_attention_decoder(\n",
    "          decoder_inputs,\n",
    "          encoder_state,\n",
    "          attention_states,\n",
    "          cell,\n",
    "          target_vocab_size,\n",
    "          size,\n",
    "          num_heads=num_heads,\n",
    "          output_size=output_size,\n",
    "          output_projection=output_projection,\n",
    "          feed_previous=feed_previous,\n",
    "          initial_state_attention=initial_state_attention)\n",
    "      return outputs, state, latent_z, latent_loss\n",
    "\n",
    "    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n",
    "    def decoder(feed_previous_bool):\n",
    "      reuse = None if feed_previous_bool else True\n",
    "      with variable_scope.variable_scope(\n",
    "          variable_scope.get_variable_scope(), reuse=reuse) as scope:\n",
    "        outputs, state = embedding_attention_decoder(\n",
    "            decoder_inputs,\n",
    "            encoder_state,\n",
    "            attention_states,\n",
    "            cell,\n",
    "            target_vocab_size,\n",
    "            size,\n",
    "            num_heads=num_heads,\n",
    "            output_size=output_size,\n",
    "            output_projection=output_projection,\n",
    "            feed_previous=feed_previous_bool,\n",
    "            update_embedding_for_previous=False,\n",
    "            initial_state_attention=initial_state_attention)\n",
    "        state_list = [state]\n",
    "        if nest.is_sequence(state):\n",
    "          state_list = nest.flatten(state)\n",
    "        return outputs + state_list\n",
    "\n",
    "    outputs_and_state = control_flow_ops.cond(feed_previous,\n",
    "                                              lambda: decoder(True),\n",
    "                                              lambda: decoder(False))\n",
    "    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n",
    "    state_list = outputs_and_state[outputs_len:]\n",
    "    state = state_list[0]\n",
    "    if nest.is_sequence(encoder_state):\n",
    "      state = nest.pack_sequence_as(structure=encoder_state,\n",
    "                                    flat_sequence=state_list)\n",
    "  return outputs_and_state[:outputs_len], state, latent_z, latent_loss\n",
    "\n",
    "def embedding_rnn_seq2seq(encoder_inputs, decoder_inputs, cell,\n",
    "                          output_projection=None,\n",
    "                          feed_previous=False, dtype=dtypes.float32,\n",
    "                          scope=None,):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "  with variable_scope.variable_scope(scope or \"embedding_rnn_seq2seq\"):\n",
    "    # Encoder.\n",
    "    encoder_cell = rnn_cell.EmbeddingWrapper(\n",
    "        cell, embedding_classes=source_vocab_size,\n",
    "        embedding_size=size)\n",
    "    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n",
    "    \n",
    "    # Patrick: keep your eyes on that encoder state\n",
    "    encoder_state, latent_z, latent_loss = make_vae(encoder_state)\n",
    "\n",
    "    # Decoder.\n",
    "    if output_projection is None:\n",
    "      cell = rnn_cell.OutputProjectionWrapper(cell, target_vocab_size)\n",
    "\n",
    "    if isinstance(feed_previous, bool):\n",
    "      outputs, state = embedding_rnn_decoder(\n",
    "          decoder_inputs, encoder_state, cell, target_vocab_size,\n",
    "          size, output_projection=output_projection,\n",
    "          feed_previous=feed_previous)\n",
    "      return outputs, state, latent_z, latent_loss\n",
    "\n",
    "    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n",
    "    def decoder(feed_previous_bool):\n",
    "      reuse = None if feed_previous_bool else True\n",
    "      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                         reuse=reuse):\n",
    "        outputs, state = embedding_rnn_decoder(\n",
    "            decoder_inputs, encoder_state, cell, target_vocab_size,\n",
    "            size, output_projection=output_projection,\n",
    "            feed_previous=feed_previous_bool,\n",
    "            update_embedding_for_previous=False)\n",
    "        state_list = [state]\n",
    "        if nest.is_sequence(state):\n",
    "          state_list = nest.flatten(state)\n",
    "        return outputs + state_list\n",
    "\n",
    "    outputs_and_state = control_flow_ops.cond(feed_previous,\n",
    "                                              lambda: decoder(True),\n",
    "                                              lambda: decoder(False))\n",
    "    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n",
    "    state_list = outputs_and_state[outputs_len:]\n",
    "    state = state_list[0]\n",
    "    if nest.is_sequence(encoder_state):\n",
    "      state = nest.pack_sequence_as(structure=encoder_state, flat_sequence=state_list)\n",
    "    \n",
    "    return outputs_and_state[:outputs_len], state, latent_z, latent_loss\n",
    "  \n",
    "def model_with_buckets(encoder_inputs, decoder_inputs, targets, weights,\n",
    "                       buckets, feed_previous, cell, output_projection=None,\n",
    "                       softmax_loss_function=None, name=None):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "\n",
    "  all_inputs = encoder_inputs + decoder_inputs + targets + weights\n",
    "  losses = []\n",
    "  outputs = []\n",
    "  \n",
    "  latent_losses = []\n",
    "  latent_zs = []\n",
    "  \n",
    "  with ops.op_scope(all_inputs, name, \"model_with_buckets\"):\n",
    "    for j, bucket in enumerate(buckets):\n",
    "      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                         reuse=True if j > 0 else None):\n",
    "        bucket_outputs, _, latent_z, latent_loss = embedding_attention_seq2seq(\n",
    "            encoder_inputs[:bucket[0]],\n",
    "            decoder_inputs[:bucket[1]],\n",
    "            cell,\n",
    "            output_projection=output_projection,\n",
    "            feed_previous=feed_previous)\n",
    "        \n",
    "        outputs.append(bucket_outputs)\n",
    "        \n",
    "        latent_losses.append(tf.reduce_mean(latent_loss))\n",
    "        latent_zs.append(latent_z)\n",
    "        \n",
    "        losses.append(sequence_loss(\n",
    "            outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n",
    "            softmax_loss_function=softmax_loss_function))\n",
    "\n",
    "  return outputs, losses, latent_losses, latent_zs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The translation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "  def __init__(self,\n",
    "               buckets,\n",
    "               size,\n",
    "               num_layers,\n",
    "               max_gradient_norm,\n",
    "               batch_size,\n",
    "               learning_rate,\n",
    "               learning_rate_decay_factor,\n",
    "               use_lstm=False,\n",
    "               num_samples=512,\n",
    "               forward_only=False,\n",
    "               dtype=tf.float32):\n",
    "    \n",
    "    self.buckets = buckets\n",
    "    self.learning_rate = tf.Variable(\n",
    "        float(learning_rate), trainable=False, dtype=dtype)\n",
    "    self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "        self.learning_rate * learning_rate_decay_factor)\n",
    "    self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # If we use sampled softmax, we need an output projection.\n",
    "    output_projection = None\n",
    "    softmax_loss_function = None\n",
    "    # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "    if num_samples > 0 and num_samples < target_vocab_size:\n",
    "      w_t = tf.get_variable(\"proj_w\", [target_vocab_size, size], dtype=dtype)\n",
    "      w = tf.transpose(w_t)\n",
    "      b = tf.get_variable(\"proj_b\", [target_vocab_size], dtype=dtype)\n",
    "      output_projection = (w, b)\n",
    "\n",
    "      def sampled_loss(inputs, labels):\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "        # avoid numerical instabilities.\n",
    "        local_w_t = tf.cast(w_t, tf.float32)\n",
    "        local_b = tf.cast(b, tf.float32)\n",
    "        local_inputs = tf.cast(inputs, tf.float32)\n",
    "        return tf.cast(\n",
    "            tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n",
    "                                       num_samples, target_vocab_size),dtype)\n",
    "      \n",
    "      softmax_loss_function = sampled_loss\n",
    "\n",
    "    # Create the internal multi-layer cell for our RNN.\n",
    "    single_cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "    if use_lstm:\n",
    "      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n",
    "    cell = single_cell\n",
    "    if num_layers > 1:\n",
    "      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "    # Feeds for inputs.\n",
    "    self.encoder_inputs = []\n",
    "    self.decoder_inputs = []\n",
    "    self.target_weights = []\n",
    "    for i in range(buckets[-1][0]):  # Last bucket is the biggest one.\n",
    "      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"encoder{0}\".format(i)))\n",
    "    for i in range(buckets[-1][1] + 1):\n",
    "      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"decoder{0}\".format(i)))\n",
    "      self.target_weights.append(tf.placeholder(dtype, shape=[None],\n",
    "                                                name=\"weight{0}\".format(i)))\n",
    "\n",
    "    # Our targets are decoder inputs shifted by one.\n",
    "    targets = [self.decoder_inputs[i + 1] for i in range(len(self.decoder_inputs) - 1)]\n",
    "    \n",
    "    # Training outputs and losses.\n",
    "    if forward_only:\n",
    "      self.outputs, self.losses, self.latent_losses, self.latent_zs = model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets,\n",
    "          feed_previous=True,\n",
    "          cell=cell,\n",
    "          softmax_loss_function=softmax_loss_function,\n",
    "          output_projection=output_projection\n",
    "      )\n",
    "      # If we use output projection, we need to project outputs for decoding.\n",
    "      if output_projection is not None:\n",
    "        for b in range(len(buckets)):\n",
    "          self.outputs[b] = [\n",
    "              tf.matmul(output, output_projection[0]) + output_projection[1]\n",
    "              for output in self.outputs[b]\n",
    "          ]\n",
    "    else:\n",
    "      self.outputs, self.losses, self.latent_losses, self.latent_zs = model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets,\n",
    "          feed_previous=False,\n",
    "          cell=cell,\n",
    "          softmax_loss_function=softmax_loss_function,\n",
    "          output_projection=output_projection)\n",
    "\n",
    "    # Gradients and SGD update operation for training the model.\n",
    "    params = tf.trainable_variables()\n",
    "    if not forward_only:\n",
    "      self.gradient_norms = []\n",
    "      self.updates = []\n",
    "      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "      for b in range(len(buckets)):\n",
    "        gradients = tf.gradients(self.losses[b] + (alph_factor * self.latent_losses[b]), params)\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
    "                                                         max_gradient_norm)\n",
    "        self.gradient_norms.append(norm)\n",
    "        self.updates.append(opt.apply_gradients(\n",
    "            zip(clipped_gradients, params), global_step=self.global_step))\n",
    "\n",
    "    self.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
    "           bucket_id, forward_only):\n",
    "\n",
    "    # encoder and decoder sizes must match the corresponding bucket\n",
    "    encoder_size = len(encoder_inputs)\n",
    "    decoder_size = len(decoder_inputs)\n",
    "\n",
    "    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "    input_feed = {}\n",
    "    for l in range(encoder_size):\n",
    "      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "    for l in range(decoder_size):\n",
    "      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "      input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "    # Since our targets are decoder inputs shifted by one, we need one more.\n",
    "    last_target = self.decoder_inputs[decoder_size].name\n",
    "    input_feed[last_target] = np.zeros([batch_size], dtype=np.int32)\n",
    "\n",
    "    # Output feed: depends on whether we do a backward step or not.\n",
    "    if not forward_only:\n",
    "      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
    "                     self.gradient_norms[bucket_id],  # Gradient norm.\n",
    "                     self.losses[bucket_id],\n",
    "                     self.latent_losses[bucket_id],\n",
    "                     self.latent_zs[bucket_id]\n",
    "                    ]  \n",
    "    else:\n",
    "      output_feed = [self.losses[bucket_id],\n",
    "                     self.latent_losses[bucket_id],\n",
    "                     self.latent_zs[bucket_id]]  # Loss for this batch.\n",
    "      \n",
    "      for l in range(decoder_size):  # Output logits.\n",
    "        output_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "    outputs = session.run(output_feed, input_feed)\n",
    "    \n",
    "    # this seems like ugly code, my bad ;(\n",
    "    if not forward_only:\n",
    "      return outputs[1], outputs[2], None, outputs[3], outputs[4]\n",
    "    else:\n",
    "      return None, outputs[0], outputs[3:], outputs[1], outputs[2]\n",
    "\n",
    "    \n",
    "  def get_batch(self, data, bucket_id, sample=True):\n",
    "\n",
    "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "    encoder_inputs, decoder_inputs = [], []\n",
    "\n",
    "    if sample:\n",
    "      # Get a random batch of encoder and decoder inputs from data,\n",
    "      # pad them if needed, reverse encoder inputs and add GO to decoder.\n",
    "      for i in range(batch_size):\n",
    "        encoder_input, decoder_input = random.choice(data[bucket_id])\n",
    "\n",
    "        # Encoder inputs are padded and then reversed.\n",
    "        encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n",
    "        encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "\n",
    "        # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
    "        decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
    "        decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n",
    "                              [data_utils.PAD_ID] * decoder_pad_size)\n",
    "    else:\n",
    "      for i in range(batch_size):\n",
    "        encoder_input, decoder_input = data[bucket_id][i]\n",
    "\n",
    "        # Encoder inputs are padded and then reversed.\n",
    "        encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n",
    "        encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "\n",
    "        # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
    "        decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
    "        decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n",
    "                              [data_utils.PAD_ID] * decoder_pad_size)\n",
    "\n",
    "    # Now we create batch-major vectors from the data selected above.\n",
    "    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "\n",
    "    # Batch encoder inputs are just re-indexed encoder_inputs.\n",
    "    for length_idx in range(encoder_size):\n",
    "      batch_encoder_inputs.append(\n",
    "          np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in range(batch_size)], dtype=np.int32))\n",
    "\n",
    "    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
    "    for length_idx in range(decoder_size):\n",
    "      batch_decoder_inputs.append(\n",
    "          np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in range(batch_size)], dtype=np.int32))\n",
    "\n",
    "      # Create target_weights to be 0 for targets that are padding.\n",
    "      batch_weight = np.ones(batch_size, dtype=np.float32)\n",
    "      for batch_idx in range(batch_size):\n",
    "        # We set weight to 0 if the corresponding target is a PAD symbol.\n",
    "        # The corresponding target is decoder_input shifted by 1 forward.\n",
    "        if length_idx < decoder_size - 1:\n",
    "          target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n",
    "          batch_weight[batch_idx] = 0.0\n",
    "      batch_weights.append(batch_weight)\n",
    "    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n",
    "  \n",
    "def make_dir(directory):\n",
    "  if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "  return directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  reading data line 100000\n",
      "  reading data line 200000\n",
      "  reading data line 300000\n",
      "  reading data line 400000\n",
      "  reading data line 500000\n",
      "  reading data line 600000\n",
      "  reading data line 700000\n",
      "  reading data line 800000\n",
      "  reading data line 900000\n",
      "  reading data line 1000000\n",
      "  reading data line 1100000\n",
      "  reading data line 1200000\n",
      "  reading data line 1300000\n",
      "  reading data line 1400000\n",
      "  reading data line 1500000\n",
      "  reading data line 1600000\n",
      "  reading data line 1700000\n",
      "  reading data line 1800000\n",
      "  reading data line 1900000\n",
      "  reading data line 2000000\n",
      "  reading data line 2100000\n",
      "  reading data line 2200000\n",
      "  reading data line 2300000\n",
      "  reading data line 2400000\n",
      "  reading data line 2500000\n",
      "  reading data line 2600000\n",
      "  reading data line 2700000\n",
      "  reading data line 2800000\n",
      "  reading data line 2900000\n",
      "  reading data line 3000000\n",
      "  reading data line 3100000\n",
      "  reading data line 3200000\n",
      "  reading data line 3300000\n",
      "  reading data line 3400000\n",
      "  reading data line 3500000\n",
      "  reading data line 3600000\n",
      "  reading data line 3700000\n",
      "  reading data line 3800000\n",
      "  reading data line 3900000\n",
      "  reading data line 4000000\n",
      "  reading data line 4100000\n",
      "  reading data line 4200000\n",
      "  reading data line 4300000\n",
      "  reading data line 4400000\n",
      "  reading data line 4500000\n",
      "  reading data line 4600000\n",
      "  reading data line 4700000\n",
      "  reading data line 4800000\n",
      "  reading data line 4900000\n",
      "  reading data line 5000000\n",
      "  reading data line 5100000\n",
      "  reading data line 5200000\n",
      "  reading data line 5300000\n",
      "  reading data line 5400000\n",
      "  reading data line 5500000\n",
      "  reading data line 5600000\n",
      "  reading data line 5700000\n",
      "  reading data line 5800000\n",
      "  reading data line 5900000\n",
      "Creating 2 layers of 36 units.\n",
      "Created model with fresh parameters.\n",
      "Reading development and training data (limit: 6100000).\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n",
      "logging summaries and errors in: seq2seq_train_protein/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5c7917fdccb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m         train_set, bucket_id)\n\u001b[1;32m    185\u001b[0m     _, step_loss, _, latent_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n\u001b[0;32m--> 186\u001b[0;31m                                  target_weights, bucket_id, False)\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mstep_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_per_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_per_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2a3294b1167e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0moutput_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# this seems like ugly code, my bad ;(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Desktop/seq2seq/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Desktop/seq2seq/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Desktop/seq2seq/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ubuntu/Desktop/seq2seq/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Desktop/seq2seq/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alph_factor = 1\n",
    "learning_rate = 0.5\n",
    "learning_rate_decay_factor = 0.99\n",
    "max_gradient_norm = 2.0 # control exploding gradients !!\n",
    "batch_size = 32\n",
    "size = 32 # both the embedding size and the RNN state size\n",
    "n_z = 24\n",
    "num_layers = 2\n",
    "source_vocab_size = 10000\n",
    "target_vocab_size = 10000\n",
    "train_dir = make_dir(\"seq2seq_train_protein/\")\n",
    "max_train_data_size = 6100000\n",
    "steps_per_checkpoint = 1500\n",
    "do_self_test = False\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See Seq2SeqModel for details of how they work.\n",
    "_buckets = [(20, 20), (30, 30), (40, 40), (50, 50)]\n",
    "\n",
    "def read_data(source_path, target_path, max_size=None):\n",
    "  \"\"\"taken (with modification from tensorflow) so look up docstrings elsewhere\"\"\"\n",
    "  data_set = [[] for _ in _buckets]\n",
    "  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
    "    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
    "      source, target = source_file.readline(), target_file.readline()\n",
    "      counter = 0\n",
    "      while source and target and (not max_size or counter < max_size):\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  reading data line %d\" % counter)\n",
    "          sys.stdout.flush()\n",
    "        source_ids = [int(x) for x in source.split()]\n",
    "        target_ids = [int(x) for x in target.split()]\n",
    "        target_ids.append(data_utils.EOS_ID)\n",
    "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "            data_set[bucket_id].append([source_ids, target_ids])\n",
    "            break\n",
    "        source, target = source_file.readline(), target_file.readline()\n",
    "  return data_set\n",
    "\n",
    "def create_model(session, forward_only):\n",
    "  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "  model = Seq2SeqModel(\n",
    "      _buckets,\n",
    "      size,\n",
    "      num_layers,\n",
    "      max_gradient_norm,\n",
    "      batch_size,\n",
    "      learning_rate,\n",
    "      learning_rate_decay_factor,\n",
    "      forward_only=forward_only)\n",
    "\n",
    "  ckpt = tf.train.get_checkpoint_state(train_dir)\n",
    "  if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "  return model\n",
    "      \n",
    "def write_batch_input_output_prediction_to_file(n_examples=5, n_sample=3):\n",
    "  # note_ n_examples must be less than or equal to the batch size\n",
    "  s = \"\"\n",
    "  input_seqs = []\n",
    "  enc_inputs_npa = np.array([list(ar) for ar in encoder_inputs]).T\n",
    "  \n",
    "  for sent_id in range(batch_size):\n",
    "    sent_in = enc_inputs_npa[sent_id]\n",
    "    str_input_seq = str(sent_id) + \": \" + \" \".join(\n",
    "      reversed([rev_vocab[n].decode(\"utf8\") for n in sent_in if n != 0])\n",
    "    ) + \"\\n\"\n",
    "    input_seqs.append(str_input_seq)\n",
    "    \n",
    "  for sent_id in range(n_examples):\n",
    "    # print n_examples of \"real\" inputs\n",
    "    sent_in = enc_inputs_npa[sent_id]\n",
    "    s += input_seqs[sent_id]\n",
    "    \n",
    "    # for each \"real\" input, print n_sample samples\n",
    "    for sample_id in range(n_sample):\n",
    "      _, eval_loss, output_logits, latent_loss, latent_z = model.step(sess, encoder_inputs,\n",
    "                                                 decoder_inputs, target_weights, bucket_id, True)\n",
    "      pred_outputs = np.array([list(np.argmax(logit, axis=1)) for logit in output_logits]).T\n",
    "      s += str(sent_id) + \": \" + \" \".join([rev_vocab[n].decode(\"utf8\") for n in pred_outputs[sent_id] if n != 0]) + \"\\n\"\n",
    "\n",
    "\n",
    "  with open(train_dir + \"batch_trans_\" + str(int(g_step)) + \"_bucket_\" + str(bucket_id) + \".txt\", \"w\") as f:\n",
    "    f.write(s)\n",
    "    \n",
    "  # print log of latent space (over the test batch)\n",
    "  df = pd.DataFrame(latent_z, index=input_seqs)\n",
    "  df.to_csv(train_dir + \"latent_z_{}_{}.txt\".format(bucket_id, g_step), sep=\" \", header=False)\n",
    "\n",
    "\n",
    "if do_self_test:\n",
    "  source_vocab_size = 10\n",
    "  target_vocab_size = 10\n",
    "  \n",
    "  \"\"\"Test the translation model.\"\"\"\n",
    "  sess = tf.Session() # make new session (no loading of saved session)\n",
    "  print(\"Self-test for neural translation model.\")\n",
    "  # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "  model = Seq2SeqModel(\n",
    "      [(3, 3),(6, 6)], # buckets (max_en, max_fr)\n",
    "      512, # size\n",
    "      2, # num layers\n",
    "      5.0, # max gradient norm\n",
    "      batch_size, # batch size\n",
    "      0.3, # learning rate\n",
    "      0.99, # learning rate decay factor\n",
    "      use_lstm=False,\n",
    "      num_samples=8, # number of samples for sampled softmax\n",
    "      forward_only=False, # run 1-directional rnn (or something like that)\n",
    "      )\n",
    "  sess.run(tf.initialize_all_variables())\n",
    "\n",
    "  data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "              [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "  for _ in range(10):  # Train the fake model for 5 steps.\n",
    "    bucket_id = random.choice([0, 1]) # choose a bucket\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "    _, step_loss, output_logits, latent_loss, latent_z = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "    print(\"step_loss: {}, latent_loss: {}, latent_z: {}\".format(step_loss, latent_loss, latent_z))\n",
    "\n",
    "else:\n",
    "  sess = tf.Session()\n",
    "\n",
    "  def init_vocabs():\n",
    "    data_file = \"protein_data/protein.txt\"\n",
    "    dev_data_file = \"protein_data/protein_dev.txt\"\n",
    "    vocab_file = \"protein_data/vocab.txt\"\n",
    "\n",
    "    dev_token_file = \"protein_data/dev_token.txt\"\n",
    "    data_token_file = \"protein_data/token.txt\"\n",
    "\n",
    "    # use same data as input and output (autoencoder)\n",
    "    create_vocabulary(vocab_file, data_file, source_vocab_size, None)\n",
    "    data_to_token_ids(dev_data_file, dev_token_file, vocab_file, None)\n",
    "    data_to_token_ids(data_file, data_token_file, vocab_file, None)\n",
    "    \n",
    "    dev_set = read_data(dev_token_file, dev_token_file)\n",
    "    train_set = read_data(data_token_file, data_token_file, max_train_data_size)\n",
    "    vocab, rev_vocab = initialize_vocabulary(vocab_file)\n",
    "\n",
    "    return vocab, rev_vocab, dev_set, train_set\n",
    "\n",
    "  vocab, rev_vocab, dev_set, train_set = init_vocabs()\n",
    "  \n",
    "  sess = tf.Session()\n",
    "  # Create model.\n",
    "  print(\"Creating %d layers of %d units.\" % (num_layers, size))\n",
    "  model = create_model(sess, False)\n",
    "\n",
    "  # Read data into buckets and compute their sizes.\n",
    "  print (\"Reading development and training data (limit: %d).\"\n",
    "         % max_train_data_size)\n",
    "#   dev_set = read_data(en_dev, fr_dev)\n",
    "#   train_set = read_data(en_train, fr_train, max_train_data_size)\n",
    "  train_bucket_sizes = [len(train_set[b]) for b in range(len(_buckets))]\n",
    "  train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "  # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "  # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "  # the size if i-th training bucket, as used later.\n",
    "  train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                         for i in range(len(train_bucket_sizes))]\n",
    "\n",
    "  # This is the training loop.\n",
    "  step_time, loss, l_loss = 0.0, 0.0, 0.0\n",
    "  current_step = 0\n",
    "  previous_losses = []\n",
    "  while True:\n",
    "    # Choose a bucket according to data distribution. We pick a random number\n",
    "    # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "    random_number_01 = np.random.random_sample()\n",
    "    bucket_id = min([i for i in range(len(train_buckets_scale))\n",
    "                     if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "    # Get a batch and make a step.\n",
    "    start_time = time.time()\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "        train_set, bucket_id)\n",
    "    _, step_loss, _, latent_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                 target_weights, bucket_id, False)\n",
    "    step_time += (time.time() - start_time) / steps_per_checkpoint\n",
    "    loss += step_loss / steps_per_checkpoint\n",
    "    l_loss += latent_loss / steps_per_checkpoint\n",
    "\n",
    "    # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "    if current_step % steps_per_checkpoint == 0: # --------------------------------------\n",
    "      print(\"logging summaries and errors in: \" + train_dir)\n",
    "      # Print statistics for the previous epoch.\n",
    "      perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "      g_step = sess.run(model.global_step)\n",
    "      l_rate = sess.run(model.learning_rate)\n",
    "      with open(train_dir + \"error_train_log.txt\", \"a\") as f:\n",
    "        f.write(\"train {} {} {}\\n\".format(l_rate, loss, l_loss))\n",
    "      # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "      if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "        sess.run(model.learning_rate_decay_op)\n",
    "      previous_losses.append(loss)\n",
    "      # Save checkpoint and zero timer and loss.\n",
    "      checkpoint_path = os.path.join(train_dir, \"translate.ckpt\")\n",
    "      model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "      step_time, loss, l_loss = 0.0, 0.0, 0.0\n",
    "      # Run evals on development set and print their perplexity. --------------------------\n",
    "      for bucket_id in range(len(_buckets)):\n",
    "        # want to take a (deterministic) sample of whole bucket\n",
    "        temp_batch_size = batch_size\n",
    "        batch_size = len(dev_set[bucket_id])\n",
    "        if len(dev_set[bucket_id]) == 0:\n",
    "          with open(train_dir + \"error_log.txt\", \"a\") as f:\n",
    "            f.write(\"test {} empty empty\\n\".format(bucket_id))\n",
    "          continue\n",
    "\n",
    "        encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id, sample=False)\n",
    "        _, eval_loss, output_logits, latent_loss, latent_z = model.step(sess, encoder_inputs,\n",
    "                                                 decoder_inputs, target_weights, bucket_id, True)\n",
    "        \n",
    "        eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n",
    "        with open(train_dir + \"error_test_log.txt\", \"a\") as f:\n",
    "          f.write(\"test {} {} {}\\n\".format(bucket_id, eval_loss, latent_loss))\n",
    "        \n",
    "        # print text example translations (note: this should be last thing in test because we are going to do stuff)\n",
    "        write_batch_input_output_prediction_to_file()\n",
    "        \n",
    "      # print out encoder embeddings as pandas dframe --------------------------------------\n",
    "      for var in tf.trainable_variables():\n",
    "        if var.name == 'embedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding:0':\n",
    "          input_embedd_op = var\n",
    "      \n",
    "      norm = tf.sqrt(tf.reduce_sum(tf.square(input_embedd_op), 1, keep_dims=True))\n",
    "      normalized_embeddings = input_embedd_op / norm\n",
    "      input_embedd_ar = sess.run(normalized_embeddings)\n",
    "      \n",
    "      token_strs = [rev_vocab[i].decode(\"utf8\") for i in range(len(rev_vocab))]\n",
    "      df = pd.DataFrame(data=input_embedd_ar[:len(rev_vocab),:], columns=list(range(input_embedd_ar.shape[1])), index=token_strs)\n",
    "      df.to_csv(train_dir + \"embedd_df_\" + str(int(g_step)) + \".csv\", sep=\" \")\n",
    "      \n",
    "      # reset batch size\n",
    "      batch_size = temp_batch_size\n",
    "      \n",
    "      # -------------------------------------------------------------------------------------\n",
    "    current_step += 1\n",
    "\n",
    "    # break to capture environment for debug\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train bucket sizes: [389984, 1119938, 2240222, 2138382]\n",
      "test bucket sizes: [94, 270, 533, 450]\n"
     ]
    }
   ],
   "source": [
    "# description of data\n",
    "\n",
    "# distribution of read lengths and how many in each bucket\n",
    "print(\"train bucket sizes: {}\".format(train_bucket_sizes))\n",
    "print(\"test bucket sizes: {}\".format(list([len(dev_set[i]) for i in range(len(dev_set))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 10.45118976 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Memory used: \" + str(process.memory_info().rss/1000000000) + \" GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAGHCAYAAAAdnkAlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f41cd3a40f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAGHCAYAAAC06oixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njL9rrzC8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f41cd3c0978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for bucket_id in range(len(dev_set)):\n",
    "  bucket_len = []\n",
    "  for seq_id in range(len(dev_set[bucket_id])):\n",
    "    bucket_len.append(len(dev_set[bucket_id][seq_id][0]))\n",
    "  plt.hist(np.array(bucket_len))\n",
    "  plt.title(\"Testing Set Peptide Lengths\")\n",
    "  plt.xlabel(\"Length\")\n",
    "  plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "  \n",
    "for bucket_id in range(len(train_set)):\n",
    "  bucket_len = []\n",
    "  for seq_id in range(len(train_set[bucket_id])):\n",
    "    bucket_len.append(len(train_set[bucket_id][seq_id][0]))\n",
    "  plt.hist(np.array(bucket_len))\n",
    "  plt.title(\"Training Set Peptide Lengths\")\n",
    "  plt.xlabel(\"Length\")\n",
    "  plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# def xavier_init(fan_in, fan_out, constant=1): \n",
    "#     \"\"\" Xavier initialization of network weights\"\"\"\n",
    "#     # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "#     low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "#     high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "#     return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "  \n",
    "# n_z = 10\n",
    "# n_input = 5\n",
    "# batch_size = 16\n",
    "\n",
    "# x = tf.placeholder(tf.float32, shape=(batch_size, n_input))\n",
    "# # x = tf.Variable(xavier_init(batch_size, n_input))\n",
    "  \n",
    "# b_mean = tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "\n",
    "# w_mean = tf.Variable(xavier_init(n_input, n_z))\n",
    "  \n",
    "# z_mean = tf.add(tf.matmul(x, w_mean), b_mean)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# # make data\n",
    "# x_feed =  np.random.normal(loc=0.0, scale=1.0, size=(batch_size, n_input))\n",
    "# x_feed\n",
    "\n",
    "# # test out example graph\n",
    "# # sess.run(z_mean, feed_dict={x: x_feed}).shape\n",
    "# z_mean.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "270\n",
      "533\n",
      "450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1347"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for bucket_id in range(len(dev_set)):\n",
    "  print(len(dev_set[bucket_id]))\n",
    "sum([94, 270, 533, 450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
