{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependancies, parameters, and setup\n",
    "# important paper do not forget:\n",
    "# https://arxiv.org/pdf/1610.02415v1.pdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "train_dir = \"/home/ubuntu/Desktop/seq2seq/seq2seq_train_protein_z16/\"\n",
    "data_dir = \"/home/ubuntu/Desktop/seq2seq/protein_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# error training summaries\n",
    "\n",
    "# training error\n",
    "df_train_er = pd.read_csv(train_dir + \"error_train_log.txt\",\n",
    "                          names=[\"\", \"global step\", \"learning rate\", \"step time\", \"loss\", \"latent loss\"],\n",
    "                          sep=\" \",\n",
    "                          index_col=False\n",
    "                         )\n",
    "# step time\n",
    "# df_train_er.plot(y=3, title=\"train step time\")\n",
    "\n",
    "# loss\n",
    "# df_train_er.plot(y=4, title=\"train loss\")\n",
    "\n",
    "# latent_loss\n",
    "# df_train_er.plot(y=5, title=\"train latent loss\")\n",
    "\n",
    "# testing error by bucket\n",
    "df_test_er = pd.read_csv(train_dir + \"error_test_log.txt\",\n",
    "                          names=[\"\", \"bucket_id\", \"loss\", \"latent loss\"],\n",
    "                          sep=\" \",\n",
    "                          index_col=False\n",
    "                         )\n",
    "df_test_er.index = list(range(1500,(df_test_er.shape[0]+1)*1500, 1500))\n",
    "\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True, sharey=False)\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "for bucket_id in range(4):\n",
    "  ax = axes[bucket_id]\n",
    "  ax = df_test_er[df_test_er.bucket_id == bucket_id][[\"loss\", \"latent loss\"]].plot(\n",
    "    title=\"bucket id: \" + str(bucket_id), ax=ax, figsize=(8, 6))\n",
    "  ax.set_xlabel('number of batches', fontsize=10)\n",
    "  ax.set_ylabel('log perplexity', fontsize = 10)\n",
    "  ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "  ax.ticklabel_format(axis='both', style='sci', scilimits=(-4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize embeddings\n",
    "\n",
    "# embeddings after batch 1,500\n",
    "latent_df = pd.read_csv(train_dir + \"embedd_df_1500.csv\", sep=\" \")\n",
    "ax = sns.clustermap(cosine_similarity(latent_df.iloc[:,1:].values), figsize=(8, 6))\n",
    "\n",
    "# embeddings after batch 171,000\n",
    "latent_df = pd.read_csv(train_dir + \"embedd_df_171000.csv\", sep=\" \")\n",
    "ax = sns.clustermap(cosine_similarity(latent_df.iloc[:,1:].values), figsize=(8, 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize latent space distribution (by bucket)\n",
    "latent_z = pd.read_csv(train_dir + \"latent_z_3_138000.txt\", sep=\" \", header=None)\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "x, y = model.fit_transform(latent_z).T\n",
    "plt.scatter(x, y)\n",
    "# latent_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example translations\n",
    "for bucket_id in range(4):\n",
    "  print(\"Bucket \" + str(bucket_id) + \": \")\n",
    "  with open(train_dir + \"batch_trans_144000_bucket_{}.txt\".format(bucket_id)) as f_in:\n",
    "    for l in f_in:\n",
    "      tokens = l.strip().split(\" \")\n",
    "      last_tokens = [t for t in tokens[1:] if t not in [\"_EOS\", \"_GO\"]]\n",
    "      s = \"\".join(last_tokens).replace(\"_UNK\", \"_\")\n",
    "      print(tokens[0] + \" \" + s)\n",
    "  print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
